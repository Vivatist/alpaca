# Ollama –Ω–∞ —É–¥–∞–ª—ë–Ω–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ —Å GPU
# –ó–∞–ø—É—Å–∫: docker compose -f docker-compose.ollama.yml up -d

name: alpaca-ollama

services:
  ollama:
    image: ollama/ollama:latest
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_NUM_GPU=1
      - OLLAMA_GPU_LAYERS=999
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_QUEUE=10
      # –†–∞–∑—Ä–µ—à–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏–∑–≤–Ω–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–æ–ª—å–∫–æ localhost)
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [compute,utility]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Warmup –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä - –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ –≤ VRAM –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
  ollama-warmup:
    image: curlimages/curl:latest
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "‚è≥ Warming up Ollama models..."
        # –ó–∞–≥—Ä—É–∂–∞–µ–º embedding –º–æ–¥–µ–ª—å
        curl -s http://ollama:11434/api/embeddings -d '{"model":"bge-m3","prompt":"warmup"}' > /dev/null
        echo "‚úÖ bge-m3 loaded"
        # –ó–∞–≥—Ä—É–∂–∞–µ–º LLM –º–æ–¥–µ–ª—å
        curl -s http://ollama:11434/api/generate -d '{"model":"qwen2.5:32b","prompt":"Hi","stream":false}' > /dev/null
        echo "‚úÖ qwen2.5:32b loaded"
        echo "üöÄ All models ready in VRAM!"
        # –î–µ—Ä–∂–∏–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –≤ —Å–ø—è—â–µ–º —Ä–µ–∂–∏–º–µ —á—Ç–æ–±—ã Docker –Ω–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–ª –µ–≥–æ
        sleep infinity

volumes:
  ollama_storage:
