#!/bin/bash
# =============================================================================
# ALPACA - –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama –≤ Docker –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
# =============================================================================
# –≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç Ollama –≤ Docker –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è ALPACA:
#   - bge-m3: –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (1024 –∏–∑–º–µ—Ä–µ–Ω–∏–π)
#   - qwen2.5:32b: LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
#
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
#   ./setup_ollama.sh           # –ó–∞–ø—É—Å—Ç–∏—Ç—å Ollama –≤ Docker + –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏
#   ./setup_ollama.sh --models  # –¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ (–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω)
#   ./setup_ollama.sh --check   # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É
#   ./setup_ollama.sh --stop    # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
#
# –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
#   - Docker –∏ docker-compose
#   - –î–ª—è GPU: NVIDIA –¥—Ä–∞–π–≤–µ—Ä—ã –∏ nvidia-container-toolkit
# =============================================================================

set -e

# –¶–≤–µ—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# –ú–æ–¥–µ–ª–∏ –¥–ª—è ALPACA
EMBEDDING_MODEL="bge-m3"
CHAT_MODEL="qwen2.5:32b"

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ docker-compose.ollama.yml
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
COMPOSE_FILE="$SCRIPT_DIR/docker-compose.ollama.yml"

# –§—É–Ω–∫—Ü–∏–∏ –≤—ã–≤–æ–¥–∞
info() { echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"; }
success() { echo -e "${GREEN}‚úÖ $1${NC}"; }
warning() { echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"; }
error() { echo -e "${RED}‚ùå $1${NC}"; exit 1; }

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
check_gpu() {
    if command -v nvidia-smi &> /dev/null; then
        info "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ NVIDIA GPU:"
        nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
        HAS_GPU=true
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ nvidia-container-toolkit
        if docker info 2>/dev/null | grep -q "nvidia"; then
            success "nvidia-container-toolkit –Ω–∞—Å—Ç—Ä–æ–µ–Ω"
        else
            warning "nvidia-container-toolkit –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ Docker"
            info "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"
        fi
    else
        warning "NVIDIA GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞. Ollama –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU."
        HAS_GPU=false
    fi
}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ docker-compose —Ñ–∞–π–ª–∞
check_compose_file() {
    if [[ ! -f "$COMPOSE_FILE" ]]; then
        error "–ù–µ –Ω–∞–π–¥–µ–Ω $COMPOSE_FILE"
    fi
}

# –ó–∞–ø—É—Å–∫ Ollama –≤ Docker
start_ollama() {
    info "–ó–∞–ø—É—Å–∫ Ollama –≤ Docker..."
    
    check_compose_file
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ —É–∂–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
    if docker ps --format '{{.Names}}' | grep -q "alpaca-ollama"; then
        success "Ollama –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω"
        return 0
    fi
    
    docker compose -f "$COMPOSE_FILE" up -d
    
    info "–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ Ollama..."
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è
    for i in {1..30}; do
        if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            success "Ollama –∑–∞–ø—É—â–µ–Ω–∞ –≤ Docker"
            return 0
        fi
        echo -n "."
        sleep 2
    done
    echo ""
    
    error "Ollama –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –ø–æ—Å–ª–µ 60 —Å–µ–∫—É–Ω–¥. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏: docker logs alpaca-ollama-ollama-1"
}

# –û—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama
stop_ollama() {
    info "–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama..."
    
    check_compose_file
    
    docker compose -f "$COMPOSE_FILE" down
    
    success "Ollama –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"
}

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Docker exec
pull_model() {
    local model=$1
    local description=$2
    
    info "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ $model ($description)..."
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –º–æ–¥–µ–ª—å
    local existing=$(docker exec alpaca-ollama-ollama-1 ollama list 2>/dev/null || echo "")
    if echo "$existing" | grep -q "$model"; then
        success "–ú–æ–¥–µ–ª—å $model —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        return 0
    fi
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ—Ä–µ–∑ docker exec
    if docker exec alpaca-ollama-ollama-1 ollama pull "$model"; then
        success "–ú–æ–¥–µ–ª—å $model –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
    else
        error "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å $model"
    fi
}

# –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è ALPACA
pull_alpaca_models() {
    info "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è ALPACA..."
    echo ""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω
    if ! docker ps --format '{{.Names}}' | grep -q "alpaca-ollama"; then
        error "Ollama –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ ./setup_ollama.sh"
    fi
    
    pull_model "$EMBEDDING_MODEL" "—ç–º–±–µ–¥–¥–∏–Ω–≥–∏, 1024 –∏–∑–º–µ—Ä–µ–Ω–∏–π"
    echo ""
    
    pull_model "$CHAT_MODEL" "LLM –¥–ª—è —á–∞—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"
    echo ""
    
    success "–í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!"
    echo ""
    
    info "–°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:"
    docker exec alpaca-ollama-ollama-1 ollama list
}

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
verify_installation() {
    echo ""
    info "–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏..."
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
    if docker ps --format '{{.Names}}' | grep -q "alpaca-ollama"; then
        success "Ollama –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω"
    else
        warning "Ollama –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω"
        return 1
    fi
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ API
    if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        error "Ollama API –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ http://localhost:11434"
    fi
    success "Ollama API –¥–æ—Å—Ç—É–ø–µ–Ω"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π
    local models=$(docker exec alpaca-ollama-ollama-1 ollama list 2>/dev/null || echo "")
    
    if echo "$models" | grep -q "$EMBEDDING_MODEL"; then
        success "–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ($EMBEDDING_MODEL) –¥–æ—Å—Ç—É–ø–Ω–∞"
    else
        warning "–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ($EMBEDDING_MODEL) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    fi
    
    if echo "$models" | grep -q "$CHAT_MODEL"; then
        success "LLM –º–æ–¥–µ–ª—å ($CHAT_MODEL) –¥–æ—Å—Ç—É–ø–Ω–∞"
    else
        warning "LLM –º–æ–¥–µ–ª—å ($CHAT_MODEL) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    fi
    
    # –¢–µ—Å—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    info "–¢–µ—Å—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤..."
    local emb_response=$(curl -s http://localhost:11434/api/embeddings \
        -d "{\"model\": \"$EMBEDDING_MODEL\", \"prompt\": \"—Ç–µ—Å—Ç\"}" 2>/dev/null)
    
    if echo "$emb_response" | grep -q "embedding"; then
        local emb_size=$(echo "$emb_response" | python3 -c "import sys,json; print(len(json.load(sys.stdin)['embedding']))" 2>/dev/null || echo "?")
        success "–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä–∞–±–æ—Ç–∞—é—Ç (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: $emb_size)"
    else
        warning "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–º–æ–¥–µ–ª—å –≤–æ–∑–º–æ–∂–Ω–æ –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)"
    fi
    
    echo ""
    success "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!"
    echo ""
    info "–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å ALPACA —Å–µ—Ä–≤–∏—Å–∞–º–∏:"
    echo "  –ù–∞ —ç—Ç–æ–π –º–∞—à–∏–Ω–µ:   OLLAMA_BASE_URL=http://host.docker.internal:11434"
    echo "  –° –¥—Ä—É–≥–æ–π –º–∞—à–∏–Ω—ã:  OLLAMA_BASE_URL=http://$(hostname -I | awk '{print $1}'):11434"
}

# –ü–æ–∫–∞–∑–∞—Ç—å —Å–ø—Ä–∞–≤–∫—É
show_help() {
    echo "ALPACA - –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama –≤ Docker"
    echo ""
    echo "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: $0 [–û–ü–¶–ò–Ø]"
    echo ""
    echo "–û–ø—Ü–∏–∏:"
    echo "  (–±–µ–∑ –æ–ø—Ü–∏–π)   –ó–∞–ø—É—Å—Ç–∏—Ç—å Ollama –≤ Docker –∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏"
    echo "  --models      –¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ (–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω)"
    echo "  --check       –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É"
    echo "  --stop        –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä Ollama"
    echo "  --help        –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É"
    echo ""
    echo "–ú–æ–¥–µ–ª–∏ –¥–ª—è ALPACA:"
    echo "  - $EMBEDDING_MODEL: –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (1024 –∏–∑–º–µ—Ä–µ–Ω–∏–π)"
    echo "  - $CHAT_MODEL: LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"
    echo ""
    echo "–§–∞–π–ª—ã:"
    echo "  - $COMPOSE_FILE"
}

# =============================================================================
# MAIN
# =============================================================================

main() {
    echo ""
    echo "ü¶ô ALPACA - –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama (Docker)"
    echo "======================================="
    echo ""
    
    case "${1:-}" in
        --help|-h)
            show_help
            exit 0
            ;;
        --models)
            pull_alpaca_models
            verify_installation
            ;;
        --check)
            check_gpu
            verify_installation
            ;;
        --stop)
            stop_ollama
            ;;
        "")
            check_gpu
            start_ollama
            pull_alpaca_models
            verify_installation
            ;;
        *)
            error "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ–ø—Ü–∏—è: $1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏."
            ;;
    esac
}

main "$@"
