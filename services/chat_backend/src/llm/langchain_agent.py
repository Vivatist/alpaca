"""
LangChain Agent RAG - –∞–≥–µ–Ω—Ç—Å–∫–∏–π RAG —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.
–ê–≥–µ–Ω—Ç —Å–∞–º —Ä–µ—à–∞–µ—Ç –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–†–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ MCP-—Å–µ—Ä–≤–µ—Ä (Model Context Protocol) –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
MCP_SERVER_URL –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–∫–∞–∑–∞–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.

–î–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–µ–∂–¥—É –æ–±—ã—á–Ω—ã–º RAG –∏ –∞–≥–µ–Ω—Ç—Å–∫–∏–º:
1. –í settings –¥–æ–±–∞–≤–∏—Ç—å LLM_BACKEND=langchain_agent
2. –£–∫–∞–∑–∞—Ç—å MCP_SERVER_URL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é http://localhost:8083)
"""

import os
from typing import Optional, Iterator, List, Dict, Any
from dataclasses import dataclass

import httpx

from logging_config import get_logger
from settings import settings

logger = get_logger("chat_backend.llm.langchain_agent")

# –õ–µ–Ω–∏–≤—ã–π –∏–º–ø–æ—Ä—Ç LangChain (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
_langchain_available = None


def _check_langchain():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LangChain –∏ –∫—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
    global _langchain_available
    if _langchain_available is None:
        try:
            from langchain_ollama import ChatOllama
            from langchain_core.messages import HumanMessage, SystemMessage, AIMessageChunk
            from langchain_core.tools import tool
            from langgraph.prebuilt import create_react_agent
            _langchain_available = True
        except ImportError as e:
            logger.warning(f"LangChain not available: {e}")
            _langchain_available = False
    return _langchain_available


@dataclass
class AgentConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞."""
    model: str = None
    base_url: str = None
    temperature: float = 0.7
    max_tokens: int = 2048
    mcp_server_url: str = None  # URL MCP-—Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
    
    def __post_init__(self):
        self.model = self.model or getattr(settings, 'OLLAMA_LLM_MODEL', 'qwen2.5:32b')
        self.base_url = self.base_url or getattr(settings, 'OLLAMA_BASE_URL', 'http://ollama:11434')
        # MCP-—Å–µ—Ä–≤–µ—Ä: –∏–∑ settings, ENV –∏–ª–∏ localhost –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.mcp_server_url = (
            self.mcp_server_url 
            or getattr(settings, 'MCP_SERVER_URL', None)
            or os.getenv('MCP_SERVER_URL', 'http://localhost:8083')
        )


def _search_via_mcp(query: str, mcp_url: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ MCP-—Å–µ—Ä–≤–µ—Ä.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        mcp_url: URL MCP-—Å–µ—Ä–≤–µ—Ä–∞
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å content, metadata, similarity
    """
    try:
        with httpx.Client(timeout=30.0) as client:
            response = client.post(
                f"{mcp_url}/tools/search_documents",
                json={"query": query, "top_k": top_k}
            )
            response.raise_for_status()
            data = response.json()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç MCP DocumentChunk –≤ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç
            chunks = []
            for c in data.get("chunks", []):
                chunks.append({
                    "content": c.get("content", ""),
                    "metadata": {
                        "file_path": c.get("file_path", ""),
                        "file_name": c.get("file_name", ""),
                        "title": c.get("title"),
                        "summary": c.get("summary"),
                        "category": c.get("category"),
                        "chunk_index": c.get("chunk_index", 0),
                    },
                    "similarity": c.get("similarity", 0),
                })
            
            logger.debug(f"MCP search '{query[:30]}...' ‚Üí {len(chunks)} chunks")
            return chunks
            
    except httpx.HTTPStatusError as e:
        logger.error(f"MCP HTTP error: {e.response.status_code} - {e.response.text}")
    except httpx.RequestError as e:
        logger.error(f"MCP request error: {e}")
    except Exception as e:
        logger.error(f"MCP search error: {e}")
    
    return []


def _create_search_tool(config: AgentConfig):
    """–°–æ–∑–¥–∞—ë—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ MCP."""
    from langchain_core.tools import tool
    
    @tool
    def search_documents(query: str) -> str:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏.
        –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö,
        –¥–æ–≥–æ–≤–æ—Ä–∞—Ö, –ø–∏—Å—å–º–∞—Ö –∏–ª–∏ –¥—Ä—É–≥–æ–π –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ
            
        Returns:
            –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not config.mcp_server_url:
            return "–û—à–∏–±–∫–∞: MCP_SERVER_URL –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"
        
        chunks = _search_via_mcp(query, config.mcp_server_url, top_k=5)
        
        if not chunks:
            return "–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è LLM
        results = []
        for i, chunk in enumerate(chunks, 1):
            metadata = chunk.get("metadata", {})
            file_path = metadata.get("file_path", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫")
            title = metadata.get("title") or metadata.get("file_name") or file_path
            content = chunk.get("content", "")[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            similarity = chunk.get("similarity", 0)
            
            result = f"[–î–æ–∫—É–º–µ–Ω—Ç {i}] {title} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.2f})\n{content}"
            results.append(result)
        
        logger.info(f"üîç Agent search: '{query[:30]}...' ‚Üí {len(results)} results")
        return "\n\n---\n\n".join(results)
    
    return search_documents


def _create_agent(config: AgentConfig):
    """–°–æ–∑–¥–∞—ë—Ç LangChain –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏."""
    from langchain_ollama import ChatOllama
    from langgraph.prebuilt import create_react_agent
    
    # –°–æ–∑–¥–∞—ë–º LLM
    llm = ChatOllama(
        model=config.model,
        base_url=config.base_url,
        temperature=config.temperature,
        num_predict=config.max_tokens,
    )
    
    # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ config –¥–ª—è MCP
    tools = [_create_search_tool(config)]
    
    # –°–æ–∑–¥–∞—ë–º –∞–≥–µ–Ω—Ç–∞
    agent = create_react_agent(llm, tools)
    
    return agent


def generate_response(
    prompt: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2048
) -> str:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LangChain –∞–≥–µ–Ω—Ç–∞.
    
    –ê–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (–ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤) –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
    """
    if not _check_langchain():
        logger.error("LangChain not available, falling back to empty response")
        return ""
    
    try:
        from langchain_core.messages import HumanMessage, SystemMessage
        
        config = AgentConfig(temperature=temperature, max_tokens=max_tokens)
        agent = _create_agent(config)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = []
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=prompt))
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≥–µ–Ω—Ç–∞
        result = agent.invoke({"messages": messages})
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        final_messages = result.get("messages", [])
        if final_messages:
            return final_messages[-1].content
        
        return ""
        
    except Exception as e:
        logger.error(f"LangChain agent error: {e}")
        return ""


def generate_response_stream(
    prompt: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2048
) -> Iterator[str]:
    """
    –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LangChain –∞–≥–µ–Ω—Ç–∞.
    
    Yields:
        –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è —à–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞
    """
    if not _check_langchain():
        logger.error("LangChain not available")
        yield "–û—à–∏–±–∫–∞: LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
        return
    
    try:
        from langchain_core.messages import HumanMessage, SystemMessage, AIMessageChunk
        
        config = AgentConfig(temperature=temperature, max_tokens=max_tokens)
        agent = _create_agent(config)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = []
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=prompt))
        
        # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞
        for event in agent.stream({"messages": messages}, stream_mode="messages"):
            # event —ç—Ç–æ tuple (message, metadata)
            if isinstance(event, tuple) and len(event) >= 1:
                message = event[0]
                
                # AIMessageChunk —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
                if hasattr(message, 'content') and message.content:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º tool calls, —Å—Ç—Ä–∏–º–∏–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
                    if not hasattr(message, 'tool_calls') or not message.tool_calls:
                        yield message.content
        
        logger.info("LangChain agent stream completed")
        
    except Exception as e:
        logger.error(f"LangChain agent stream error: {e}")
        yield f"–û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞: {e}"


# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
__all__ = [
    "generate_response",
    "generate_response_stream",
    "AgentConfig",
]
