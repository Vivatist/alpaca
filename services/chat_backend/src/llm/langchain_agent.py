"""
LangChain Agent RAG - –∞–≥–µ–Ω—Ç—Å–∫–∏–π RAG —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.
–ê–≥–µ–Ω—Ç —Å–∞–º —Ä–µ—à–∞–µ—Ç –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:
1. –° –≤–Ω–µ–¥—Ä—ë–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ–∏—Å–∫–∞ (set_search_function) - –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å pipeline
2. –° MCP-—Å–µ—Ä–≤–µ—Ä–æ–º (MCP_SERVER_URL env) - –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã

–î–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –º–µ–∂–¥—É –æ–±—ã—á–Ω—ã–º RAG –∏ –∞–≥–µ–Ω—Ç—Å–∫–∏–º:
1. –í settings –¥–æ–±–∞–≤–∏—Ç—å LLM_BACKEND=langchain_agent
2. –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–ø—Ä—è–º—É—é: from llm.langchain_agent import generate_response_stream
"""

import os
from typing import Optional, Iterator, List, Dict, Any, Callable
from dataclasses import dataclass

from logging_config import get_logger
from settings import settings

logger = get_logger("chat_backend.llm.langchain_agent")

# –õ–µ–Ω–∏–≤—ã–π –∏–º–ø–æ—Ä—Ç LangChain (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω)
_langchain_available = None


def _check_langchain():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LangChain –∏ –∫—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
    global _langchain_available
    if _langchain_available is None:
        try:
            from langchain_ollama import ChatOllama
            from langchain_core.messages import HumanMessage, SystemMessage, AIMessageChunk
            from langchain_core.tools import tool
            from langgraph.prebuilt import create_react_agent
            _langchain_available = True
        except ImportError as e:
            logger.warning(f"LangChain not available: {e}")
            _langchain_available = False
    return _langchain_available


@dataclass
class AgentConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞."""
    model: str = None
    base_url: str = None
    temperature: float = 0.7
    max_tokens: int = 2048
    mcp_server_url: str = None  # URL MCP-—Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
    
    def __post_init__(self):
        self.model = self.model or getattr(settings, 'OLLAMA_LLM_MODEL', 'qwen2.5:32b')
        self.base_url = self.base_url or getattr(settings, 'OLLAMA_BASE_URL', 'http://ollama:11434')
        self.mcp_server_url = self.mcp_server_url or os.getenv('MCP_SERVER_URL')


# –¢–∏–ø –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ (–∏–Ω—ä–µ–∫—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)
SearchFunction = Callable[[str], List[Dict[str, Any]]]

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ (—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –∏–∑–≤–Ω–µ)
_search_function: Optional[SearchFunction] = None


def set_search_function(fn: SearchFunction):
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–∞.
    
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ pipeline –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
    
    Args:
        fn: –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞, –ø—Ä–∏–Ω–∏–º–∞–µ—Ç query –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
    """
    global _search_function
    _search_function = fn
    logger.info("Search function registered for agent")


def _search_via_mcp(query: str, mcp_url: str) -> List[Dict[str, Any]]:
    """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ MCP-—Å–µ—Ä–≤–µ—Ä."""
    import httpx
    
    try:
        with httpx.Client(timeout=30.0) as client:
            response = client.post(
                f"{mcp_url}/tools/search_documents",
                json={"query": query, "top_k": 5}
            )
            if response.status_code == 200:
                data = response.json()
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç MCP –≤ —Ñ–æ—Ä–º–∞—Ç chunks
                return [
                    {
                        "content": c["content"],
                        "metadata": {
                            "file_path": c["file_path"],
                            "title": c.get("title"),
                            "summary": c.get("summary"),
                            "category": c.get("category"),
                            "chunk_index": c.get("chunk_index", 0),
                        },
                        "similarity": c.get("similarity", 0),
                    }
                    for c in data.get("chunks", [])
                ]
    except Exception as e:
        logger.error(f"MCP search error: {e}")
    return []


def _create_search_tool(config: AgentConfig = None):
    """–°–æ–∑–¥–∞—ë—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–∞."""
    from langchain_core.tools import tool
    
    @tool
    def search_documents(query: str) -> str:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏.
        –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö,
        –¥–æ–≥–æ–≤–æ—Ä–∞—Ö, –ø–∏—Å—å–º–∞—Ö –∏–ª–∏ –¥—Ä—É–≥–æ–π –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ
            
        Returns:
            –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        chunks = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –≤–Ω–µ–¥—Ä—ë–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è > MCP-—Å–µ—Ä–≤–µ—Ä
        if _search_function is not None:
            try:
                chunks = _search_function(query)
            except Exception as e:
                logger.error(f"Search function error: {e}")
        elif config and config.mcp_server_url:
            chunks = _search_via_mcp(query, config.mcp_server_url)
        else:
            return "–û—à–∏–±–∫–∞: —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –∏ MCP-—Å–µ—Ä–≤–µ—Ä –Ω–µ —É–∫–∞–∑–∞–Ω"
        
        if not chunks:
            return "–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for i, chunk in enumerate(chunks, 1):
            metadata = chunk.get("metadata", {})
            file_path = metadata.get("file_path", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫")
            title = metadata.get("title", "")
            content = chunk.get("content", "")[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            similarity = chunk.get("similarity", 0)
            
            result = f"[–î–æ–∫—É–º–µ–Ω—Ç {i}] {title or file_path} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.2f})\n{content}"
            results.append(result)
        
        logger.info(f"üîç Agent search: '{query[:30]}...' ‚Üí {len(results)} results")
        return "\n\n---\n\n".join(results)
    
    return search_documents


def _create_agent(config: AgentConfig):
    """–°–æ–∑–¥–∞—ë—Ç LangChain –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏."""
    from langchain_ollama import ChatOllama
    from langgraph.prebuilt import create_react_agent
    
    # –°–æ–∑–¥–∞—ë–º LLM
    llm = ChatOllama(
        model=config.model,
        base_url=config.base_url,
        temperature=config.temperature,
        num_predict=config.max_tokens,
    )
    
    # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ config –¥–ª—è MCP
    tools = [_create_search_tool(config)]
    
    # –°–æ–∑–¥–∞—ë–º –∞–≥–µ–Ω—Ç–∞
    agent = create_react_agent(llm, tools)
    
    return agent


def generate_response(
    prompt: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2048
) -> str:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LangChain –∞–≥–µ–Ω—Ç–∞.
    
    –ê–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (–ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤) –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
    """
    if not _check_langchain():
        logger.error("LangChain not available, falling back to empty response")
        return ""
    
    try:
        from langchain_core.messages import HumanMessage, SystemMessage
        
        config = AgentConfig(temperature=temperature, max_tokens=max_tokens)
        agent = _create_agent(config)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = []
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=prompt))
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≥–µ–Ω—Ç–∞
        result = agent.invoke({"messages": messages})
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        final_messages = result.get("messages", [])
        if final_messages:
            return final_messages[-1].content
        
        return ""
        
    except Exception as e:
        logger.error(f"LangChain agent error: {e}")
        return ""


def generate_response_stream(
    prompt: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 2048
) -> Iterator[str]:
    """
    –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LangChain –∞–≥–µ–Ω—Ç–∞.
    
    Yields:
        –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ –º–µ—Ä–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è —à–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞
    """
    if not _check_langchain():
        logger.error("LangChain not available")
        yield "–û—à–∏–±–∫–∞: LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
        return
    
    try:
        from langchain_core.messages import HumanMessage, SystemMessage, AIMessageChunk
        
        config = AgentConfig(temperature=temperature, max_tokens=max_tokens)
        agent = _create_agent(config)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages = []
        if system_prompt:
            messages.append(SystemMessage(content=system_prompt))
        messages.append(HumanMessage(content=prompt))
        
        # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞
        for event in agent.stream({"messages": messages}, stream_mode="messages"):
            # event —ç—Ç–æ tuple (message, metadata)
            if isinstance(event, tuple) and len(event) >= 1:
                message = event[0]
                
                # AIMessageChunk —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
                if hasattr(message, 'content') and message.content:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º tool calls, —Å—Ç—Ä–∏–º–∏–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
                    if not hasattr(message, 'tool_calls') or not message.tool_calls:
                        yield message.content
        
        logger.info("LangChain agent stream completed")
        
    except Exception as e:
        logger.error(f"LangChain agent stream error: {e}")
        yield f"–û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞: {e}"


# –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
__all__ = [
    "generate_response",
    "generate_response_stream",
    "set_search_function",
    "AgentConfig",
]
