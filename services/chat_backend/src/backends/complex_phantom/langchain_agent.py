"""
LangChain Agent –¥–ª—è Complex Phantom Backend.

ReAct –∞–≥–µ–Ω—Ç –∫–æ—Ç–æ—Ä—ã–π —Å–∞–º —Ä–µ—à–∞–µ—Ç –∫–æ–≥–¥–∞ –∏—Å–∫–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã.
–ù–∞ –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é.
"""
from typing import Any, List, Dict, Callable
from dataclasses import dataclass, field

from logging_config import get_logger

logger = get_logger("chat_backend.complex_phantom.langchain")


DEFAULT_SYSTEM_PROMPT = """–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ–º–ø–∞–Ω–∏–∏ ALPACA. 
–£ —Ç–µ–±—è –µ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç search_documents –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏.

–ü—Ä–∞–≤–∏–ª–∞:
1. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π search_documents
2. –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ–±—â–∏–π –∏–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–∏—Å–∫–∞ ‚Äî –æ—Ç–≤–µ—á–∞–π –Ω–∞–ø—Ä—è–º—É—é (–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞, —Ñ–∞–∫—Ç—ã, –ø–µ—Ä–µ–≤–æ–¥ –∏ —Ç.–¥.)
3. –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö ‚Äî –∏—â–∏ —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
4. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
5. –ù–ï –ø–µ—Ä–µ—á–∏—Å–ª—è–π –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–≤–µ—Ç–µ ‚Äî –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–≤–∏–¥–∏—Ç –∏—Ö –∫–∞–∫ —Å—Å—ã–ª–∫–∏ –ø–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏–µ–º
6. –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –±–µ–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–π–ª–æ–≤"""


@dataclass
class SearchContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    chunks: List[Dict[str, Any]] = field(default_factory=list)
    
    def clear(self):
        self.chunks = []
    
    def add_chunks(self, chunks: List[Dict[str, Any]]):
        self.chunks.extend(chunks)


def check_langchain() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LangChain."""
    try:
        from langchain_ollama import ChatOllama
        from langgraph.prebuilt import create_react_agent
        return True
    except ImportError:
        return False


def create_search_tool(
    search_func: Callable[[str, int], List[Dict[str, Any]]],
    context: SearchContext
):
    """
    –°–æ–∑–¥–∞—ë—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–∞.
    
    Args:
        search_func: –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ (query, top_k) -> chunks
        context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    from langchain_core.tools import tool
    
    @tool
    def search_documents(query: str) -> str:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏.
        –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –¥–æ–≥–æ–≤–æ—Ä–∞—Ö, –ø–∏—Å—å–º–∞—Ö.
        –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏, –ø–µ—Ä–µ–≤–æ–¥–æ–≤, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ
        """
        chunks = search_func(query, 5)
        
        if not chunks:
            return "–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º chunks –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∫–∞–∫ sources
        context.add_chunks(chunks)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∞–≥–µ–Ω—Ç–∞
        summaries = []
        for i, chunk in enumerate(chunks, 1):
            meta = chunk.get("metadata", {})
            title = meta.get("title") or meta.get("file_name") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            category = meta.get("category") or "–î–æ–∫—É–º–µ–Ω—Ç"
            summary = meta.get("summary") or ""
            content_preview = chunk.get("content", "")[:300]
            
            summaries.append(f"[{i}] {category}: {title}")
            if summary:
                summaries.append(f"    –û–ø–∏—Å–∞–Ω–∏–µ: {summary}")
            summaries.append(f"    –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {content_preview}...")
        
        logger.info(f"üîç Search: '{query[:30]}...' ‚Üí {len(chunks)} documents")
        
        return (
            f"–ù–∞–π–¥–µ–Ω–æ {len(chunks)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. "
            "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞:\n\n" + 
            "\n\n".join(summaries) +
            "\n\n–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. "
            "–ù–ï –ø–µ—Ä–µ—á–∏—Å–ª—è–π –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–≤–µ—Ç–µ ‚Äî –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–≤–∏–¥–∏—Ç –∏—Ö –∫–∞–∫ —Å—Å—ã–ª–∫–∏."
        )
    
    return search_documents


def create_agent(
    base_url: str,
    model: str,
    search_func: Callable[[str, int], List[Dict[str, Any]]],
    context: SearchContext,
    temperature: float = 0.7,
    max_tokens: int = 2048
):
    """
    –°–æ–∑–¥–∞—ë—Ç LangChain –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.
    """
    from langchain_ollama import ChatOllama
    from langgraph.prebuilt import create_react_agent
    
    llm = ChatOllama(
        model=model,
        base_url=base_url,
        temperature=temperature,
        num_predict=max_tokens,
    )
    
    tools = [create_search_tool(search_func, context)]
    return create_react_agent(llm, tools)
