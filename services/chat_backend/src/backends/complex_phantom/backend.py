"""
ComplexPhantomBackend ‚Äî –∫–æ–ø–∏—è complex_agent –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç RagAgent —Å –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–º ChatBackend –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
–≤ API chat-backend —Å–µ—Ä–≤–∏—Å–∞.
"""
from typing import Iterator, List
from urllib.parse import quote

from logging_config import get_logger
from settings import settings

from ..protocol import ChatBackend, StreamEvent, SourceInfo
from .vector_store import VectorStoreAdapter
from .agent import RagAgent
from .schemas import SearchResult

logger = get_logger("chat_backend.complex_phantom")


class ComplexPhantomBackend(ChatBackend):
    """
    Complex Phantom Backend ‚Äî –∫–æ–ø–∏—è complex_agent –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.
    
    Agentic RAG —Å robust search –∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º.
    """
    
    def __init__(self):
        self._agent: RagAgent | None = None
        self._vector_store: VectorStoreAdapter | None = None
    
    @property
    def name(self) -> str:
        return "complex_phantom"
    
    def _get_vector_store(self) -> VectorStoreAdapter:
        """Lazy initialization vector store."""
        if self._vector_store is None:
            self._vector_store = VectorStoreAdapter(
                database_url=settings.DATABASE_URL,
                table_name="chunks"
            )
            logger.info("‚úÖ VectorStoreAdapter initialized")
        return self._vector_store
    
    def _get_agent(self) -> RagAgent:
        """Lazy initialization –∞–≥–µ–Ω—Ç–∞."""
        if self._agent is None:
            self._agent = RagAgent(
                vector_store=self._get_vector_store(),
                ollama_url=settings.OLLAMA_BASE_URL,
                llm_model=settings.OLLAMA_LLM_MODEL,
                embedding_model=settings.OLLAMA_EMBEDDING_MODEL,
            )
            logger.info("‚úÖ RagAgent initialized")
        return self._agent
    
    def _build_source_info(self, result: SearchResult, base_url: str) -> SourceInfo:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å SourceInfo –∏–∑ SearchResult."""
        meta = result.metadata
        file_path = meta.file_path
        file_name = file_path.split("/")[-1] if file_path else "unknown"
        
        encoded_path = quote(file_path, safe="")
        download_url = f"{base_url}/api/files/download?path={encoded_path}"
        
        return SourceInfo(
            file_path=file_path,
            file_name=file_name,
            chunk_index=meta.chunk_index,
            similarity=result.final_score,
            download_url=download_url,
            title=meta.title,
            summary=meta.summary,
            category=meta.category,
            modified_at=meta.modified_at,
        )
    
    def _build_source_from_chunk(self, chunk: dict, base_url: str) -> SourceInfo:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å SourceInfo –∏–∑ chunk dict (—Ñ–æ—Ä–º–∞—Ç MCP)."""
        metadata = chunk.get("metadata", {})
        file_path = metadata.get("file_path", "")
        file_name = file_path.split("/")[-1] if file_path else "unknown"
        
        encoded_path = quote(file_path, safe="")
        download_url = f"{base_url}/api/files/download?path={encoded_path}"
        
        return SourceInfo(
            file_path=file_path,
            file_name=file_name,
            chunk_index=metadata.get("chunk_index", 0),
            similarity=chunk.get("similarity", 0),
            download_url=download_url,
            title=metadata.get("title"),
            summary=metadata.get("summary"),
            category=metadata.get("category"),
            modified_at=metadata.get("modified_at"),
        )
    
    def _check_langchain(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LangChain."""
        try:
            from langchain_ollama import ChatOllama
            from langgraph.prebuilt import create_react_agent
            return True
        except ImportError:
            logger.warning("LangChain not available")
            return False
    
    def _create_search_func(self):
        """–°–æ–∑–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ vector_store."""
        vector_store = self._get_vector_store()
        
        def search_func(query: str, top_k: int = 5):
            """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É."""
            # –ü–æ–ª—É—á–∞–µ–º embedding
            embedding = vector_store.get_embedding(
                query, settings.OLLAMA_BASE_URL, settings.OLLAMA_EMBEDDING_MODEL
            )
            if not embedding:
                return []
            
            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ search_semantic
            results = vector_store.search_semantic(embedding, limit=top_k)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º SearchHit –≤ —Ñ–æ—Ä–º–∞—Ç chunks –¥–ª—è LangChain
            chunks = []
            for hit in results:
                # MetadataModel ‚Äî pydantic, –∏—Å–ø–æ–ª—å–∑—É–µ–º model_dump()
                meta_dict = hit.metadata.model_dump() if hasattr(hit.metadata, 'model_dump') else {}
                chunks.append({
                    "content": hit.content,
                    "metadata": meta_dict,
                    "similarity": hit.base_score,
                })
            return chunks
        
        return search_func
    
    def stream(
        self,
        query: str,
        conversation_id: str | None = None,
        base_url: str = ""
    ) -> Iterator[StreamEvent]:
        """
        –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LangChain Agent.
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–¥—Ö–æ–¥ –∏–∑ agent backend:
        - –ê–≥–µ–Ω—Ç —Å–∞–º —Ä–µ—à–∞–µ—Ç –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫
        - –ù–∞ –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã (2+2) –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞–ø—Ä—è–º—É—é
        - –ù–∞ –≤–æ–ø—Ä–æ—Å—ã –ø—Ä–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç search_documents tool
        """
        logger.info(f"üì® Complex Phantom stream: {query[:50]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º LangChain
        if not self._check_langchain():
            yield StreamEvent(type="error", data={"error": "LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"})
            return
        
        try:
            from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage
            from .langchain_agent import create_agent, SearchContext, DEFAULT_SYSTEM_PROMPT
            
            # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–±–æ—Ä–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            search_context = SearchContext()
            
            # –°–æ–∑–¥–∞—ë–º –∞–≥–µ–Ω—Ç–∞
            agent = create_agent(
                base_url=settings.OLLAMA_BASE_URL,
                model=settings.OLLAMA_LLM_MODEL,
                search_func=self._create_search_func(),
                context=search_context
            )
            
            messages = [
                SystemMessage(content=DEFAULT_SYSTEM_PROMPT),
                HumanMessage(content=query)
            ]
            
            sources_sent = False
            
            # –°—Ç—Ä–∏–º–∏–º –æ—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞
            for event in agent.stream({"messages": messages}, stream_mode="messages"):
                if isinstance(event, tuple) and len(event) >= 1:
                    message = event[0]
                    
                    # –ü–æ—Å–ª–µ tool –≤—ã–∑–æ–≤–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º sources
                    if isinstance(message, ToolMessage):
                        if search_context.chunks and not sources_sent:
                            sources = [self._build_source_from_chunk(c, base_url) for c in search_context.chunks]
                            yield StreamEvent(
                                type="metadata",
                                data={
                                    "conversation_id": conversation_id or "",
                                    "sources": [s.to_dict() for s in sources],
                                }
                            )
                            sources_sent = True
                            logger.info(f"üìé Sent {len(sources)} sources")
                        continue
                    
                    if isinstance(message, AIMessage):
                        # Tool calls
                        if hasattr(message, 'tool_calls') and message.tool_calls:
                            for tc in message.tool_calls:
                                yield StreamEvent(
                                    type="tool_call",
                                    data={"name": tc.get("name", ""), "args": tc.get("args", {})}
                                )
                        # Text content
                        elif message.content:
                            yield StreamEvent(type="chunk", data={"content": message.content})
            
            # –ï—Å–ª–∏ sources –Ω–µ –±—ã–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã ‚Äî –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            if not sources_sent:
                yield StreamEvent(
                    type="metadata",
                    data={"conversation_id": conversation_id or "", "sources": []}
                )
            
            yield StreamEvent(type="done", data={})
            
        except Exception as e:
            logger.error(f"‚ùå Complex Phantom error: {e}")
            yield StreamEvent(type="error", data={"error": str(e)})
