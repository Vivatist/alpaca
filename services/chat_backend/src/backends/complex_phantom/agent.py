"""
RAG Agent ‚Äî LangChain –∞–≥–µ–Ω—Ç —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ streaming.

RagAgent:
1. –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ (category, company, person, etc.)
2. –í—ã–∑—ã–≤–∞–µ—Ç search_documents —á–µ—Ä–µ–∑ robust_search
3. –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —á–µ—Ä–µ–∑ stream_callback
4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""
import json
import re
from typing import List, Optional, Callable, Iterator

from logging_config import get_logger
from .schemas import (
    SearchResult, ExtractedFilters, AgentAnswer, 
    RetryDebugInfo, SearchFilter
)
from .vector_store import VectorStoreAdapter
from .search_tool import create_search_tool, SearchContext
from .robust_search import robust_search, StreamCallback
from .config import (
    DOCUMENT_CATEGORIES, 
    AGENT_SYSTEM_PROMPT,
    QUERY_EXTRACTION_PROMPT,
    DEFAULT_SEARCH_LIMIT,
)

logger = get_logger("chat_backend.complex_agent.agent")


class RagAgent:
    """
    RAG –∞–≥–µ–Ω—Ç —Å LangChain, robust search –∏ streaming.
    
    –ú–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –¥–≤—É—Ö —Ä–µ–∂–∏–º–∞—Ö:
    1. –° LangChain –∞–≥–µ–Ω—Ç–æ–º (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω langchain)
    2. –ë–µ–∑ –∞–≥–µ–Ω—Ç–∞ ‚Äî –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤ search + generate
    """
    
    def __init__(
        self,
        vector_store: VectorStoreAdapter,
        ollama_url: str,
        llm_model: str,
        embedding_model: str,
        system_prompt: Optional[str] = None
    ):
        self.vector_store = vector_store
        self.ollama_url = ollama_url
        self.llm_model = llm_model
        self.embedding_model = embedding_model
        self.system_prompt = system_prompt or AGENT_SYSTEM_PROMPT
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LangChain
        self._langchain_available = self._check_langchain()
    
    def _check_langchain(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LangChain."""
        try:
            from langchain_core.messages import HumanMessage
            from langchain_ollama import ChatOllama
            return True
        except ImportError:
            logger.warning("LangChain not available, using direct mode")
            return False
    
    def answer(
        self,
        user_query: str,
        stream_callback: Optional[StreamCallback] = None
    ) -> AgentAnswer:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        
        Args:
            user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            stream_callback: Callback –¥–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            
        Returns:
            AgentAnswer —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        """
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        if stream_callback:
            stream_callback("üîé –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–ø—Ä–æ—Å...")
        
        filters = self._extract_filters(user_query)
        
        # 2. –ü–æ–ª—É—á–∞–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞
        embedding = self.vector_store.get_embedding(
            user_query, self.ollama_url, self.embedding_model
        )
        
        if not embedding:
            return AgentAnswer(
                final_text="–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å",
                used_documents=[],
                debug_info=RetryDebugInfo()
            )
        
        # 3. –í—ã–ø–æ–ª–Ω—è–µ–º robust search
        results, debug_info = robust_search(
            vector_store=self.vector_store,
            embedding=embedding,
            filters=filters.to_search_filter(),
            limit=DEFAULT_SEARCH_LIMIT,
            stream_callback=stream_callback
        )
        
        # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        if stream_callback:
            stream_callback("üí≠ –§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
        
        final_text = self._generate_answer(user_query, results)
        
        return AgentAnswer(
            final_text=final_text,
            used_documents=results,
            debug_info=debug_info
        )
    
    def stream_answer(
        self,
        user_query: str,
        stream_callback: Optional[StreamCallback] = None
    ) -> Iterator[str]:
        """
        –ü–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å.
        
        Yields:
            –ß–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        """
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        if stream_callback:
            stream_callback("üîé –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∑–∞–ø—Ä–æ—Å...")
        
        filters = self._extract_filters(user_query)
        
        # 2. –û–±–æ–≥–∞—â–∞–µ–º query –¥–ª—è semantic search
        # Entity –∏ keywords –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ query –¥–ª—è embedding (–Ω–µ SQL!)
        enriched_query = self._enrich_query(user_query, filters)
        
        # 3. Embedding –æ–±–æ–≥–∞—â—ë–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        embedding = self.vector_store.get_embedding(
            enriched_query, self.ollama_url, self.embedding_model
        )
        
        if not embedding:
            yield "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å"
            return
        
        # 4. Search (SQL —Ñ–∏–ª—å—Ç—Ä—ã: —Ç–æ–ª—å–∫–æ category –∏ date)
        results, debug_info = robust_search(
            vector_store=self.vector_store,
            embedding=embedding,
            filters=filters.to_search_filter(),
            limit=DEFAULT_SEARCH_LIMIT,
            stream_callback=stream_callback
        )
        
        if not results:
            yield "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
            return
        
        # 5. Stream generate
        if stream_callback:
            stream_callback("üí≠ –§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
        
        yield from self._stream_generate(user_query, results)
    
    def _enrich_query(self, query: str, filters: ExtractedFilters) -> str:
        """
        –û–±–æ–≥–∞—Ç–∏—Ç—å query –¥–ª—è semantic search.
        
        Entity –∏ keywords –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∫ –∑–∞–ø—Ä–æ—Å—É –¥–ª—è embedding.
        –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π—Ç–∏ "–ê–∫–ø–∞–Ω", "–ê–∫–ø–∞–Ω–û–ú", "–ê–ö–ü–ê–ù".
        
        Args:
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            filters: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
            
        Returns:
            –û–±–æ–≥–∞—â—ë–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        """
        parts = [query]
        
        if filters.entity:
            parts.append(filters.entity)
        
        if filters.keywords:
            parts.extend(filters.keywords[:3])  # –ú–∞–∫—Å 3 keywords
        
        enriched = " ".join(parts)
        logger.debug(f"Enriched query: {enriched}")
        return enriched

    def _extract_filters(self, query: str) -> ExtractedFilters:
        """
        –ò–∑–≤–ª–µ—á—å —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ LLM.
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            ExtractedFilters —Å category, entity, keywords, etc.
        """
        import requests
        
        categories_list = "\n".join(f"- {cat}" for cat in DOCUMENT_CATEGORIES)
        prompt = QUERY_EXTRACTION_PROMPT.format(
            categories=categories_list,
            query=query
        )
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.llm_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.1, "num_predict": 300}
                },
                timeout=60
            )
            
            if response.status_code != 200:
                logger.warning(f"Filter extraction failed: {response.status_code}")
                return ExtractedFilters()
            
            llm_response = response.json().get("response", "")
            return self._parse_extracted_filters(llm_response)
            
        except Exception as e:
            logger.error(f"Filter extraction error: {e}")
            return ExtractedFilters()
    
    def _parse_extracted_filters(self, response: str) -> ExtractedFilters:
        """–ü–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM."""
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_match = re.search(r'\{[\s\S]*\}', response)
            if not json_match:
                return ExtractedFilters()
            
            data = json.loads(json_match.group())
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category = data.get("category")
            if category and category not in DOCUMENT_CATEGORIES:
                category = None
            
            return ExtractedFilters(
                category=category,
                entity=data.get("entity"),  # –ï–¥–∏–Ω–æ–µ –ø–æ–ª–µ –¥–ª—è company/person
                keywords=data.get("keywords"),
                date_from=data.get("date_from"),
                date_to=data.get("date_to"),
            )
            
        except (json.JSONDecodeError, TypeError) as e:
            logger.debug(f"Filter parse error: {e}")
            return ExtractedFilters()
    
    def _generate_answer(self, query: str, results: List[SearchResult]) -> str:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        """
        import requests
        
        if not results:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context = self._build_context(results)
        
        prompt = f"""{self.system_prompt}

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–û—Ç–≤–µ—Ç (–∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É, –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):"""
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.llm_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.3, "num_predict": 1000}
                },
                timeout=120
            )
            
            if response.status_code == 200:
                return response.json().get("response", "").strip()
            else:
                logger.error(f"Generate failed: {response.status_code}")
                return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"
                
        except Exception as e:
            logger.error(f"Generate error: {e}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"
    
    def _stream_generate(
        self, 
        query: str, 
        results: List[SearchResult]
    ) -> Iterator[str]:
        """
        –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞.
        """
        import requests
        
        context = self._build_context(results)
        
        prompt = f"""{self.system_prompt}

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–û—Ç–≤–µ—Ç (–∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É, –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):"""
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.llm_model,
                    "prompt": prompt,
                    "stream": True,
                    "options": {"temperature": 0.3, "num_predict": 1000}
                },
                stream=True,
                timeout=120
            )
            
            if response.status_code != 200:
                yield "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"
                return
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        if "response" in data:
                            yield data["response"]
                        if data.get("done"):
                            break
                    except json.JSONDecodeError:
                        continue
                        
        except Exception as e:
            logger.error(f"Stream generate error: {e}")
            yield "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"
    
    def _build_context(self, results: List[SearchResult]) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è LLM.
        """
        parts = []
        
        for i, result in enumerate(results[:5], 1):  # –ú–∞–∫—Å–∏–º—É–º 5 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            meta = result.metadata
            title = meta.title or meta.file_path.split("/")[-1]
            
            part = f"[–î–æ–∫—É–º–µ–Ω—Ç {i}: {title}]"
            if meta.category:
                part += f" (–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {meta.category})"
            if meta.modified_at:
                part += f" (–¥–∞—Ç–∞: {meta.modified_at[:10]})"
            part += f"\n{result.content}"
            
            parts.append(part)
        
        return "\n\n---\n\n".join(parts)
