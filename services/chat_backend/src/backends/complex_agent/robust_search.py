"""
Robust Search ‚Äî –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å –æ—Å–ª–∞–±–ª–µ–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤.

–°—Ç—Ä–∞—Ç–µ–≥–∏—è:
1. –ò—Ç–µ—Ä–∞—Ü–∏—è 1: –í—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã (strict)
2. –ò—Ç–µ—Ä–∞—Ü–∏—è 2: –û—Å–ª–∞–±–ª–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ (—É–±–∏—Ä–∞–µ–º keywords ‚Üí company/person ‚Üí category)
3. –ò—Ç–µ—Ä–∞—Ü–∏—è 3: Fallback ‚Äî —Ç–æ–ª—å–∫–æ semantic search –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤

–ù–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è stream_callback —Å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º.
"""
from datetime import datetime, timedelta
from typing import List, Optional, Callable, Tuple, Set

from logging_config import get_logger
from .schemas import (
    SearchHit, SearchResult, SearchFilter, 
    RetryDebugInfo, MetadataModel
)
from .vector_store import VectorStoreAdapter
from .reranker import rerank_results
from .config import (
    MIN_RESULTS_THRESHOLD,
    MAX_SEARCH_ITERATIONS,
    DATE_RANGE_EXPANSION_DAYS,
    DEFAULT_SEARCH_LIMIT,
)

logger = get_logger("chat_backend.complex_agent.robust_search")


StreamCallback = Callable[[str], None]


def robust_search(
    vector_store: VectorStoreAdapter,
    embedding: List[float],
    filters: SearchFilter,
    limit: int = DEFAULT_SEARCH_LIMIT,
    stream_callback: Optional[StreamCallback] = None
) -> Tuple[List[SearchResult], RetryDebugInfo]:
    """
    –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω—ã–º –æ—Å–ª–∞–±–ª–µ–Ω–∏–µ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤.
    
    Args:
        vector_store: –ê–¥–∞–ø—Ç–µ—Ä vector store
        embedding: Embedding –∑–∞–ø—Ä–æ—Å–∞
        filters: –ù–∞—á–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        limit: –ú–∞–∫—Å–∏–º—É–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        stream_callback: Callback –¥–ª—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        
    Returns:
        (results, debug_info) ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    """
    debug = RetryDebugInfo()
    
    # –ò—Ç–µ—Ä–∞—Ü–∏—è 1: –°—Ç—Ä–æ–≥–∏–π –ø–æ–∏—Å–∫ —Å–æ –≤—Å–µ–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
    _notify(stream_callback, _describe_search(filters))
    
    results = _search_iteration(
        vector_store, embedding, filters, limit, debug,
        dropped_filters=[]
    )
    
    if len(results) >= MIN_RESULTS_THRESHOLD:
        _notify(stream_callback, f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return results, debug
    
    # –ò—Ç–µ—Ä–∞—Ü–∏—è 2: –û—Å–ª–∞–±–ª–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
    if debug.attempts < MAX_SEARCH_ITERATIONS:
        relaxed_filters, dropped = _relax_filters(filters, stream_callback)
        
        if dropped:
            results = _search_iteration(
                vector_store, embedding, relaxed_filters, limit, debug,
                dropped_filters=dropped
            )
            
            if len(results) >= MIN_RESULTS_THRESHOLD:
                _notify(stream_callback, f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –æ—Å–ª–∞–±–ª–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤")
                return results, debug
    
    # –ò—Ç–µ—Ä–∞—Ü–∏—è 3: Fallback ‚Äî —Ç–æ–ª—å–∫–æ semantic search
    if debug.attempts < MAX_SEARCH_ITERATIONS:
        _notify(stream_callback, "üîç –í—ã–ø–æ–ª–Ω—è—é —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫...")
        debug.fallback_used = True
        
        results = _search_iteration(
            vector_store, embedding, SearchFilter(), limit, debug,
            dropped_filters=["all_filters"]
        )
        
        if results:
            _notify(stream_callback, f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        else:
            _notify(stream_callback, "‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return results, debug


def _search_iteration(
    vector_store: VectorStoreAdapter,
    embedding: List[float],
    filters: SearchFilter,
    limit: int,
    debug: RetryDebugInfo,
    dropped_filters: List[str]
) -> List[SearchResult]:
    """
    –û–¥–Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞: semantic + structured ‚Üí merge ‚Üí rerank.
    """
    all_hits: List[SearchHit] = []
    seen_chunks: Set[str] = set()  # –î–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ file_path + chunk_index
    
    # 1. Semantic search
    semantic_hits = vector_store.search_semantic(
        embedding=embedding,
        limit=limit * 2,  # –ë–µ—Ä—ë–º –±–æ–ª—å—à–µ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        filters=filters if not filters.is_empty() else None
    )
    
    for hit in semantic_hits:
        key = f"{hit.metadata.file_path}:{hit.metadata.chunk_index}"
        if key not in seen_chunks:
            seen_chunks.add(key)
            all_hits.append(hit)
    
    # 2. Structured search (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã)
    if not filters.is_empty():
        structured_hits = vector_store.search_structured(
            filters=filters,
            limit=limit
        )
        
        for hit in structured_hits:
            key = f"{hit.metadata.file_path}:{hit.metadata.chunk_index}"
            if key not in seen_chunks:
                seen_chunks.add(key)
                all_hits.append(hit)
    
    # 3. Rerank
    results = rerank_results(all_hits, top_k=limit)
    
    # 4. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º debug info
    debug.add_attempt(
        used_filters=filters.to_dict(),
        dropped_filters=dropped_filters,
        message=f"Found {len(results)} results"
    )
    
    logger.info(f"Search iteration {debug.attempts}: {len(results)} results | filters={filters.to_dict()}")
    
    return results


def _relax_filters(
    filters: SearchFilter,
    stream_callback: Optional[StreamCallback]
) -> Tuple[SearchFilter, List[str]]:
    """
    –û—Å–ª–∞–±–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É.
    
    –ü–æ—Ä—è–¥–æ–∫ –æ—Å–ª–∞–±–ª–µ–Ω–∏—è:
    1. keywords (–Ω–∞–∏–º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–π)
    2. company/person (—Å—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å)
    3. category (–≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)
    4. date_from/date_to ‚Äî —Ä–∞—Å—à–∏—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω
    
    Returns:
        (relaxed_filters, dropped_filter_names)
    """
    dropped = []
    relaxed = filters.model_copy()
    
    # 1. –£–±–∏—Ä–∞–µ–º keywords
    if relaxed.keywords:
        relaxed.keywords = None
        dropped.append("keywords")
        _notify(stream_callback, "üìã –£–±–∏—Ä–∞—é —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º...")
    
    # 2. –£–±–∏—Ä–∞–µ–º company/person
    if relaxed.company:
        relaxed.company = None
        dropped.append("company")
        _notify(stream_callback, "üè¢ –£–±–∏—Ä–∞—é —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏...")
    
    if relaxed.person:
        relaxed.person = None
        dropped.append("person")
        _notify(stream_callback, "üë§ –£–±–∏—Ä–∞—é —Ñ–∏–ª—å—Ç—Ä –ø–æ –ø–µ—Ä—Å–æ–Ω–µ...")
    
    # 3. –†–∞—Å—à–∏—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
    if relaxed.date_from or relaxed.date_to:
        relaxed = _expand_date_range(relaxed, stream_callback)
        dropped.append("date_expanded")
    
    # 4. –£–±–∏—Ä–∞–µ–º category (–ø–æ—Å–ª–µ–¥–Ω–∏–π resort)
    if relaxed.category and len(dropped) < 2:
        relaxed.category = None
        dropped.append("category")
        _notify(stream_callback, "üìÅ –£–±–∏—Ä–∞—é —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏...")
    
    return relaxed, dropped


def _expand_date_range(
    filters: SearchFilter,
    stream_callback: Optional[StreamCallback]
) -> SearchFilter:
    """–†–∞—Å—à–∏—Ä–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç –Ω–∞ ¬±1 –≥–æ–¥."""
    relaxed = filters.model_copy()
    
    try:
        if relaxed.date_from:
            dt = datetime.strptime(relaxed.date_from, "%Y-%m-%d")
            new_date = dt - timedelta(days=DATE_RANGE_EXPANSION_DAYS)
            relaxed.date_from = new_date.strftime("%Y-%m-%d")
        
        if relaxed.date_to:
            dt = datetime.strptime(relaxed.date_to, "%Y-%m-%d")
            new_date = dt + timedelta(days=DATE_RANGE_EXPANSION_DAYS)
            relaxed.date_to = new_date.strftime("%Y-%m-%d")
        
        _notify(stream_callback, f"üìÖ –†–∞—Å—à–∏—Ä—è—é –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–æ {relaxed.date_from or '...'} ‚Äî {relaxed.date_to or '...'}")
        
    except ValueError:
        pass
    
    return relaxed


def _describe_search(filters: SearchFilter) -> str:
    """–°–æ–∑–¥–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞."""
    parts = ["üîç –ò—â—É –¥–æ–∫—É–º–µ–Ω—Ç—ã"]
    
    if filters.category:
        parts.append(f"–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´{filters.category}¬ª")
    
    if filters.company:
        parts.append(f"–∫–æ–º–ø–∞–Ω–∏–∏ ¬´{filters.company}¬ª")
    
    if filters.person:
        parts.append(f"—Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º ¬´{filters.person}¬ª")
    
    if filters.keywords:
        kw = ", ".join(filters.keywords[:3])
        parts.append(f"–ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {kw}")
    
    if filters.date_from or filters.date_to:
        date_range = f"{filters.date_from or '...'} ‚Äî {filters.date_to or '...'}"
        parts.append(f"–∑–∞ –ø–µ—Ä–∏–æ–¥ {date_range}")
    
    if len(parts) == 1:
        parts.append("–ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É...")
    else:
        parts[0] = "üîç –ò—â—É –¥–æ–∫—É–º–µ–Ω—Ç—ã"
    
    return " ".join(parts) + "..."


def _notify(callback: Optional[StreamCallback], message: str):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ callback –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å."""
    if callback:
        callback(message)
    logger.debug(f"Stream: {message}")
