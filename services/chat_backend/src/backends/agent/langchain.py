"""
LangChain Agent –¥–ª—è Agent Backend.

–°–æ–∑–¥–∞–Ω–∏–µ ReAct –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞.
–ê–≥–µ–Ω—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç, sources –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ.
"""
from typing import Any, List, Dict, Callable
from dataclasses import dataclass, field

from logging_config import get_logger

logger = get_logger("chat_backend.agent.langchain")


@dataclass
class SearchContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    chunks: List[Dict[str, Any]] = field(default_factory=list)
    
    def clear(self):
        self.chunks = []
    
    def add_chunks(self, chunks: List[Dict[str, Any]]):
        self.chunks.extend(chunks)


def check_langchain() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LangChain."""
    try:
        from langchain_ollama import ChatOllama
        from langgraph.prebuilt import create_react_agent
        return True
    except ImportError:
        return False


def create_search_tool(
    search_func: Callable[[str, int], List[Dict[str, Any]]],
    context: SearchContext
):
    """
    –°–æ–∑–¥–∞—ë—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –¥–ª—è –∞–≥–µ–Ω—Ç–∞.
    
    –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ context –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π 
    –ø–µ—Ä–µ–¥–∞—á–∏ –∫–∞–∫ sources (–∞ –Ω–µ –≤ —Ç–µ–∫—Å—Ç–µ –æ—Ç–≤–µ—Ç–∞).
    
    Args:
        search_func: –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ (query, top_k) -> chunks
        context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
    Returns:
        LangChain tool
    """
    from langchain_core.tools import tool
    
    @tool
    def search_documents(query: str) -> str:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏.
        –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, –¥–æ–≥–æ–≤–æ—Ä–∞—Ö, –ø–∏—Å—å–º–∞—Ö.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ
            
        Returns:
            –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞.
            –ù–ï –≤–∫–ª—é—á–∞–π —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–≤–æ–π –æ—Ç–≤–µ—Ç ‚Äî 
            –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–≤–∏–¥–∏—Ç –∏—Ö –∫–∞–∫ —Å—Å—ã–ª–∫–∏.
        """
        chunks = search_func(query, 5)
        
        if not chunks:
            return "–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º chunks –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∫–∞–∫ sources
        context.add_chunks(chunks)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∞–≥–µ–Ω—Ç–∞ (–±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
        summaries = []
        for i, chunk in enumerate(chunks, 1):
            meta = chunk.get("metadata", {})
            title = meta.get("title") or meta.get("file_name") or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            category = meta.get("category") or "–î–æ–∫—É–º–µ–Ω—Ç"
            summary = meta.get("summary") or ""
            content_preview = chunk.get("content", "")[:300]
            
            summaries.append(f"[{i}] {category}: {title}")
            if summary:
                summaries.append(f"    –û–ø–∏—Å–∞–Ω–∏–µ: {summary}")
            summaries.append(f"    –°–æ–¥–µ—Ä–∂–∏–º–æ–µ: {content_preview}...")
        
        logger.info(f"üîç Agent search: '{query[:30]}...' ‚Üí {len(chunks)} documents")
        
        return (
            f"–ù–∞–π–¥–µ–Ω–æ {len(chunks)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. "
            "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞:\n\n" + 
            "\n\n".join(summaries) +
            "\n\n–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. "
            "–ù–ï –ø–µ—Ä–µ—á–∏—Å–ª—è–π –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ç–≤–µ—Ç–µ ‚Äî –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–≤–∏–¥–∏—Ç –∏—Ö –∫–∞–∫ —Å—Å—ã–ª–∫–∏."
        )
    
    return search_documents


def create_agent(
    base_url: str,
    model: str,
    search_func: Callable[[str, int], List[Dict[str, Any]]],
    context: SearchContext,
    temperature: float = 0.7,
    max_tokens: int = 2048
):
    """
    –°–æ–∑–¥–∞—ë—Ç LangChain –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.
    
    Args:
        base_url: URL Ollama API
        model: –ú–æ–¥–µ–ª—å LLM
        search_func: –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
        
    Returns:
        LangGraph agent
    """
    from langchain_ollama import ChatOllama
    from langgraph.prebuilt import create_react_agent
    
    llm = ChatOllama(
        model=model,
        base_url=base_url,
        temperature=temperature,
        num_predict=max_tokens,
    )
    
    tools = [create_search_tool(search_func, context)]
    return create_react_agent(llm, tools)
