"""
ALPACA RAG - –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
"""
import os
import warnings

# –û—Ç–∫–ª—é—á–∞–µ–º UserWarning –î–û –ª—é–±—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤
os.environ.setdefault("PYTHONWARNINGS", "ignore::UserWarning")
warnings.filterwarnings("ignore", category=UserWarning)

import requests
import psycopg2
import psycopg2.extras
from time import sleep
from typing import Dict, List, Tuple

from app.parsers.word.parser_word import parser_word_task, parser_word_old_task

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ Prefect –î–û –∏–º–ø–æ—Ä—Ç–∞
os.environ["PREFECT_LOGGING_LEVEL"] = "WARNING"
os.environ["PREFECT_LOGGING_TO_API_ENABLED"] = "false"

from datetime import timedelta
from prefect import flow, serve, task
from pydantic import BaseModel


class FileID(BaseModel):
    """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ñ–∞–π–ª–∞ (hash + path)"""
    hash: str
    path: str
        
        
from utils.logging import setup_logging, get_logger
from utils.process_lock import ProcessLock
from app.file_watcher import FileWatcherService
from settings import settings
from database import Database

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
setup_logging()
logger = get_logger("alpaca.main")

# –°–µ—Ä–≤–∏—Å—ã
file_watcher = FileWatcherService(
    database_url=settings.DATABASE_URL,
    monitored_path=settings.MONITORED_PATH,
    allowed_extensions=settings.ALLOWED_EXTENSIONS.split(','),
    file_min_size=settings.FILE_MIN_SIZE,
    file_max_size=settings.FILE_MAX_SIZE,
    excluded_dirs=settings.EXCLUDED_DIRS.split(','),
    excluded_patterns=settings.EXCLUDED_PATTERNS.split(',')
)

db = Database(settings.DATABASE_URL)


@flow(name="file_watcher_flow")
def file_watcher_flow():
    """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤"""
    result = file_watcher.scan_and_sync()
    
    return result


@task(name="process_deleted_file", retries=2, persist_result=True)
def task_process_deleted_file(
    db: Database, file_id: FileID) -> FileID:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–∞"""
    try:
        chunks_deleted = db.delete_chunks_by_hash(file_id.hash)
        db.delete_file_by_hash(file_id.hash)
        logger.info(f"Deleted {file_id.path} and {chunks_deleted} chunks")
    except Exception as e:
        logger.error(f"ERROR when trying to delete a file {file_id.path}: {e}")
        return None
    return file_id


@task(name="chunking", retries=2)
def task_chunking(file_id: dict, text: str) -> List[str]:
    """Task: —Ä–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
    
    Args:
        file_id: dict —Å hash –∏ path
        text: —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
    Returns:
        List[str]: —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
    """
    file_id = FileID(**file_id)
    
    try:
        logger.info(f"üî™ Chunking: {file_id.path}")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ (–ø—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è - –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Ä–∞–∑–º–µ—Ä–æ–º)
        chunks = []
        max_chunk_size = 1000  # —Å–∏–º–≤–æ–ª–æ–≤
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            if len(current_chunk) + len(para) > max_chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = para
            else:
                current_chunk += "\n\n" + para if current_chunk else para
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        if not chunks:
            logger.warning(f"No chunks created for {file_id.path}")
            return []
        
        logger.info(f"‚úÖ Created {len(chunks)} chunks for {file_id.path}")
        
        return chunks
        
    except Exception as e:
        logger.error(f"Failed to chunk text | file={file_id.path} error={type(e).__name__}: {e}")
        return []


@task(name="embedding", retries=2)
def task_embedding(file_id: dict, chunks: List[str]) -> int:
    """Task: —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Ollama –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
    
    Args:
        file_id: dict —Å hash –∏ path
        chunks: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
        
    Returns:
        int: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    """
    file_id = FileID(**file_id)
    
    try:
        if not chunks:
            logger.warning(f"No chunks to embed for {file_id.path}")
            return 0
        
        logger.info(f"üîÆ Embedding {len(chunks)} chunks: {file_id.path}")
        
        # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ Ollama
        with db.get_connection() as conn:
            with conn.cursor() as cur:
                inserted_count = 0
                
                for idx, chunk_text in enumerate(chunks):
                    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –æ—Ç Ollama
                    try:
                        response = requests.post(
                            f"{settings.OLLAMA_BASE_URL}/api/embeddings",
                            json={
                                "model": settings.OLLAMA_EMBEDDING_MODEL,
                                "prompt": chunk_text
                            },
                            timeout=60
                        )
                        
                        if response.status_code != 200:
                            logger.error(f"Ollama embedding error | status={response.status_code}")
                            continue
                        
                        embedding = response.json().get('embedding')
                        
                        if not embedding:
                            logger.error(f"No embedding in response for chunk {idx}")
                            continue
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ PostgreSQL vector —Ñ–æ—Ä–º–∞—Ç
                        embedding_str = '[' + ','.join(map(str, embedding)) + ']'
                        
                        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
                        metadata = {
                            'file_hash': file_id.hash,
                            'file_path': file_id.path,
                            'chunk_index': idx,
                            'total_chunks': len(chunks)
                        }
                        
                        # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –ë–î
                        cur.execute("""
                            INSERT INTO chunks (content, metadata, embedding)
                            VALUES (%s, %s, %s::vector)
                        """, (chunk_text, psycopg2.extras.Json(metadata), embedding_str))
                        
                        inserted_count += 1
                        
                    except Exception as e:
                        logger.error(f"Error embedding chunk {idx}: {e}")
                        continue
                
                conn.commit()
        
        logger.info(f"‚úÖ Embedded {inserted_count}/{len(chunks)} chunks for {file_id.path}")
        
        return inserted_count
        
    except Exception as e:
        logger.error(f"Failed to embed chunks | file={file_id.path} error={type(e).__name__}: {e}")
        return 0




@flow(name="ingest_pipeline")
def ingest_pipeline(file_id: dict) -> str:
    """–í—Ö–æ–¥–Ω–∞—è —Ç–æ—á–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    file_id = FileID(**file_id)
    logger.info(f"üçé Start ingest pipeline: {file_id.path} (hash: {file_id.hash[:8]}...)")
    db.mark_as_processed(file_id.hash)
    
    # 1. –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª –≤ —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç
    if file_id.path.lower().endswith('.docx'):  
        raw_text = parser_word_old_task(file_id.model_dump())
    else:
        logger.error(f"Unsupported file type: {file_id.path}")
        db.mark_as_error(file_id.hash)
        return ""

    if not raw_text or not raw_text.strip():
        logger.error(f"Empty parsed text for {file_id.path}")
        db.mark_as_error(file_id.hash)
        return ""
    
    # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ temp_parsed
    temp_dir = os.path.join(os.path.dirname(__file__), "temp_parsed")
    temp_file_path = os.path.join(temp_dir, f"{file_id.path}.md")
    
    os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)
    
    with open(temp_file_path, "w", encoding="utf-8") as f:
        f.write(raw_text)
    
    # 3. –ß–∞–Ω–∫–∏–Ω–≥
    chunks = task_chunking(file_id.model_dump(), raw_text)
    
    if not chunks:
        logger.warning(f"No chunks created for {file_id.path}")
        db.mark_as_error(file_id.hash)
        return ""
    
    # 4. –≠–º–±–µ–¥–¥–∏–Ω–≥
    chunks_count = task_embedding(file_id.model_dump(), chunks)
    
    if chunks_count == 0:
        logger.warning(f"No embeddings created for {file_id.path}")
        db.mark_as_error(file_id.hash)
        return ""
    
    db.mark_as_ok(file_id.hash)
    logger.info(f"‚úÖ File processed successfully: {file_id.path} | chunks={chunks_count}")
    return ""


@flow(name="process_pending_files_flow")
def process_pending_files_flow():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤ (added/updated ‚Üí ingestion, deleted ‚Üí cleanup)"""
    pending_files = db.get_pending_files()
    total_pending = sum(len(files) for files in pending_files.values())
    logger.info(f"üìã Found {total_pending} pending files (deleted:{len(pending_files['deleted'])}, updated:{len(pending_files['updated'])}, added:{len(pending_files['added'])})")

    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –µ—Å—Ç—å –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ deleted pending-—Ñ–∞–π–ª—ã
    for file_id in pending_files['deleted']:
        task_process_deleted_file(db, file_id)

    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –µ—Å—Ç—å –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ updated pending-—Ñ–∞–π–ª—ã
    for file_id in pending_files['updated']:
        task_process_deleted_file(db, file_id)
        ingest_pipeline(file_id.model_dump())

    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –µ—Å—Ç—å –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ added pending-—Ñ–∞–π–ª—ã
    for file_id in pending_files['added']:
        ingest_pipeline(file_id.model_dump())

    return
        
        
if __name__ == "__main__":
    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–∫–∞–∫ HTTP —Å–µ—Ä–≤–µ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—Ç)
    process_lock = ProcessLock('/tmp/alpaca_rag.pid')
    process_lock.acquire()
    # process_lock.setup_handlers()  # –û—Ç–∫–ª—é—á–µ–Ω–æ: –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å Prefect Runner SIGTERM
    
    try:
        logger.info("Starting ALPACA RAG system...")
        
        # –°–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–æ–≤ processed —É —Ñ–∞–π–ª–æ–≤ –≤ –±–∞–∑–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        reset_count = file_watcher.reset_processed_statuses()
            
        # –ó–∞–ø—É—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö flows —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        serve(
            file_watcher_flow.to_deployment(
                name="file-watcher",
                interval=timedelta(seconds=settings.SCAN_MONITORED_FOLDER_INTERVAL),
                description="–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤",
                concurrency_limit=1
            ),
            process_pending_files_flow.to_deployment(
                name="process_pending_files_flow",
                interval=timedelta(seconds=settings.PROCESS_FILE_CHANGES_INTERVAL),
                description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤",
                concurrency_limit=1 # settings.MAX_HEAVY_WORKFLOWS
            )
        )
    except KeyboardInterrupt:
        logger.info("Shutting down gracefully...")
    finally:
        process_lock.release()
