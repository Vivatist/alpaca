"""
ALPACA RAG - –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
"""
import os
from typing import Dict, List, Tuple
import warnings

os.environ.setdefault("PYTHONWARNINGS", "ignore::UserWarning")
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic_settings.main")

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ Prefect –î–û –∏–º–ø–æ—Ä—Ç–∞
os.environ["PREFECT_LOGGING_LEVEL"] = "WARNING"
os.environ["PREFECT_LOGGING_TO_API_ENABLED"] = "false"

from datetime import timedelta
from prefect import flow, serve, task
from prefect.artifacts import create_table_artifact
from utils.logging import setup_logging, get_logger
from utils.process_lock import ProcessLock
from app.file_watcher import FileWatcherService
from app.flows.file_status_processor import FileStatusProcessorService
from settings import settings
from database import Database

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
setup_logging()
logger = get_logger(__name__)

# –°–µ—Ä–≤–∏—Å—ã
file_watcher = FileWatcherService(
    database_url=settings.DATABASE_URL,
    monitored_path=settings.MONITORED_PATH,
    allowed_extensions=settings.ALLOWED_EXTENSIONS.split(','),
    file_min_size=settings.FILE_MIN_SIZE,
    file_max_size=settings.FILE_MAX_SIZE,
    excluded_dirs=settings.EXCLUDED_DIRS.split(','),
    excluded_patterns=settings.EXCLUDED_PATTERNS.split(',')
)

# file_processor = FileStatusProcessorService(
#     database_url=settings.DATABASE_URL,
#     webhook_url=settings.N8N_WEBHOOK_URL,
#     max_heavy_workflows=settings.MAX_HEAVY_WORKFLOWS
# )

db = Database(settings.DATABASE_URL)


@flow(name="file_watcher_flow")
def file_watcher_flow():
    """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤"""
    result = file_watcher.scan_and_sync()
    
    return result



@task(name="process_deleted_files", retries=2, persist_result=True)
def task_process_deleted_files(
    db: Database,
    files: List[Tuple[str, str, int]]
) -> int:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–æ–≤"""
    processed = 0
    
    for file_path, file_hash, file_size in files:
        try:
            logger.info(f"Processing deleted: {file_path}")
            chunks_deleted = db.task_delete_chunks_by_hash(db, file_hash)
            db.task_delete_file(db, file_hash)
            logger.info(f"Deleted {chunks_deleted} chunks and file record")
            processed += 1
        except Exception as e:
            logger.error(f"ERROR when trying to delete a file {file_path}: {e}")
    
    return processed


@task(name="process_added_files", retries=2, persist_result=True)
def task_process_added_files(
    db: Database,
    webhook_url: str,
    files: List[Tuple[str, str, int]],
    slots_available: int
) -> Dict[str, int]:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ added —Ñ–∞–π–ª–æ–≤"""
    stats = {'processed': 0, 'skipped': 0}
    
    for file_path, file_hash, file_size in files:
        if slots_available > 0:
            try:
                logger.info(f"‚ûï Processing added: {file_path}")
                db.task_call_webhook(webhook_url, file_path, file_hash)
                db.task_mark_as_processed(db, file_hash)
                stats['processed'] += 1
                slots_available -= 1
            except Exception as e:
                logger.error(f"‚ùå Failed to process added file {file_path}: {e}")
                db.task_mark_as_error(db, file_hash)
        else:
            logger.info(f"‚è∏Ô∏è  Workflow limit reached, skipping remaining added files")
            stats['skipped'] = len(files) - stats['processed']
            break
    
    return stats


@task(name="process_updated_files", retries=2, persist_result=True)
def task_process_updated_files(db: Database, webhook_url: str, files: List[Tuple[str, str, int]], slots_available: int) -> Dict[str, int]:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ updated —Ñ–∞–π–ª–æ–≤"""
    stats = {'processed': 0, 'skipped': 0}
    
    for file_path, file_hash, file_size in files:
        if slots_available > 0:
            try:
                logger.info(f"üîÑ Processing updated: {file_path}")
                chunks_deleted = db.task_delete_chunks_by_path(db, file_path)
                logger.info(f"üóëÔ∏è  Deleted {chunks_deleted} old chunks")
                db.task_call_webhook(webhook_url, file_path, file_hash)
                db.task_mark_as_processed(db, file_hash)
                stats['processed'] += 1
                slots_available -= 1
            except Exception as e:
                logger.error(f"‚ùå Failed to process updated file {file_path}: {e}")
                db.db.task_mark_as_error(db, file_hash)
        else:
            logger.info(f"‚è∏Ô∏è  Workflow limit reached, skipping remaining updated files")
            stats['skipped'] = len(files) - stats['processed']
            break
    
    return stats


@flow(name="ingest_files_flow")
def ingest_files_flow():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤ (added/updated ‚Üí ingestion, deleted ‚Üí cleanup)"""
    logger.info("Starting file status processing flow...")
    while True:
        pending_files = db.get_pending_files()
        total_pending = sum(len(files) for files in pending_files.values())
        logger.info(f"üìã Found {total_pending} pending files (deleted:{len(pending_files['deleted'])}, updated:{len(pending_files['updated'])}, added:{len(pending_files['added'])})")
        if total_pending == 0:
            break
        
        if pending_files['deleted']:
            task_process_deleted_files(db, pending_files['deleted'])
        
        if pending_files['updated'] or pending_files['added']:
            logger.info("‚è∏Ô∏è  Skipping updated/added files (not implemented yet)")
            break  # –í—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–µ—Ä—ã–≤–∞–µ–º —Ü–∏–∫–ª, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–Ω—É—Ç—å
    result = pending_files
    return result
        
        
if __name__ == "__main__":
    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–∫–∞–∫ HTTP —Å–µ—Ä–≤–µ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—Ç)
    process_lock = ProcessLock('/tmp/alpaca_rag.pid')
    process_lock.acquire()
    # process_lock.setup_handlers()  # –û—Ç–∫–ª—é—á–µ–Ω–æ: –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å Prefect Runner SIGTERM
    
    try:
        logger.info("Starting ALPACA RAG system...")
        
        # –°–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–æ–≤ processed —É —Ñ–∞–π–ª–æ–≤ –≤ –±–∞–∑–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        reset_count = file_watcher.reset_processed_statuses()
        
        # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º setup_handlers() - –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É–µ—Ç —Å Prefect Runner
        # atexit —É–∂–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω, —ç—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
        
        # –ó–∞–ø—É—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö flows —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        serve(
            file_watcher_flow.to_deployment(
                name="file-watcher",
                interval=timedelta(seconds=settings.SCAN_MONITORED_FOLDER_INTERVAL),
                description="–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤",
                concurrency_limit=1
            ),
            ingest_files_flow.to_deployment(
                name="ingest_files_flow",
                interval=timedelta(seconds=settings.PROCESS_FILE_CHANGES_INTERVAL),
                description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤",
                concurrency_limit=1
            )
        )
    except KeyboardInterrupt:
        logger.info("Shutting down gracefully...")
    finally:
        process_lock.release()
