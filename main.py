"""
Worker - –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞, —á–∞–Ω–∫–∏–Ω–≥–∞, —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
"""
import os
from typing import Dict, Any
from threading import Semaphore

from app.parsers.word_parser_module.word_parser import WordParser
from app.chunkers.custom_chunker import chunking
from app.embedders.custom_embedder import embedding
from utils.logging import setup_logging, get_logger
from utils.worker import Worker
from settings import settings
from utils.database import PostgreDataBase
from utils.file_manager import File, FileManager
from tests.runner import run_tests_on_startup

logger = get_logger("alpaca.worker")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
db = PostgreDataBase(settings.DATABASE_URL)
fm = FileManager(db)
word_parser = WordParser(enable_ocr=True)  # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞
FILEWATCHER_API = os.getenv("FILEWATCHER_API_URL", "http://localhost:8081")

# –°–µ–º–∞—Ñ–æ—Ä—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (–∏–∑ settings)
PARSE_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_PARSING)
EMBED_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_EMBEDDING)
LLM_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_LLM)


def ingest_pipeline(file: File) -> bool:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: –ø–∞—Ä—Å–∏–Ω–≥ ‚Üí —á–∞–Ω–∫–∏–Ω–≥ ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥
    
    Args:
        file: –û–±—ä–µ–∫—Ç File —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–µ
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
    """
    logger.info(f"üçé Start ingest pipeline: {file.path} (hash: {file.hash[:8]}...)")
    
    try:
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
        if file.path.lower().endswith('.docx'):
            logger.info(f"üìñ Parsing file: {file.path}")
            with PARSE_SEMAPHORE:
                file.raw_text = word_parser.parse(file)
            logger.info(f"‚úÖ Parsed: {len(file.raw_text) if file.raw_text else 0} chars")
        else:
            logger.error(f"Unsupported file type: {file.path}")
            fm.mark_as_error(file)
            return False

        if not file.raw_text or not file.raw_text.strip():
            logger.error(f"Empty parsed text for {file.path}")
            fm.mark_as_error(file)
            return False
    
        
        # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ temp_parsed
        fm.save_file_to_disk(file)
        
        # 3. –ß–∞–Ω–∫–∏–Ω–≥
        chunks = chunking(file)
        
        if not chunks:
            logger.warning(f"No chunks created for {file.path}")
            fm.mark_as_error(file)
            return False
        
        # 4. –≠–º–±–µ–¥–¥–∏–Ω–≥ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
        with EMBED_SEMAPHORE:
            chunks_count = embedding(db, file, chunks)
        
        if chunks_count == 0:
            logger.warning(f"No embeddings created for {file.path}")
            fm.mark_as_error(file)
            return False
        
        fm.mark_as_ok(file)
        logger.info(f"‚úÖ File processed successfully: {file.path} | chunks={chunks_count}")
        return True
        
    except Exception as e:
        import traceback
        logger.error(f"Pipeline failed for {file.path}: {e}")
        logger.error(f"Traceback:\n{traceback.format_exc()}")
        fm.mark_as_error(file)
        return False


def process_file(file_info: Dict[str, Any]) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª
    
    Args:
        file_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –∏–∑ filewatcher
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
    """
    # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç File –∏–∑ —Å–ª–æ–≤–∞—Ä—è
    file = File(**file_info)
    
    logger.info(f"Processing file: {file.path} (status={file.status_sync})")
    
    try:
        if file.status_sync == 'deleted':
            # –£–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏ –∏ —Ñ–∞–π–ª –∏–∑ –ë–î
            fm.delete_file_and_chunks(file)
            return True
            
        elif file.status_sync == 'updated':
            # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏, —Ñ–∞–π–ª –æ—Å—Ç–∞—ë—Ç—Å—è –≤ –ë–î
            fm.delete_chunks_only(file)
            return ingest_pipeline(file)
            
        elif file.status_sync == 'added':
            # –ù–æ–≤—ã–π —Ñ–∞–π–ª - –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            return ingest_pipeline(file)
            
        else:
            logger.warning(f"Unknown status: {file.status_sync} for {file.path}")
            return False
            
    except Exception as e:
        logger.error(f"‚úó Error processing {file.path}: {e}")
        fm.mark_as_error(file)
        return False

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö)
    tests_passed = run_tests_on_startup(settings)

    if not tests_passed:
        exit(1)

    # –ü–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º logging –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤ (pytest –º–æ–∂–µ—Ç –∑–∞–∫—Ä—ã—Ç—å handlers)
    setup_logging()
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ worker –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤")

    # –°–æ–∑–¥–∞—ë–º worker –∏ –∑–∞–ø—É—Å–∫–∞–µ–º
    worker = Worker(
        db = db,
        filewatcher_api_url=FILEWATCHER_API,
        process_file_func=process_file # –ø–µ—Ä–µ–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫–æ—Ç–æ—Ä—É—é –±—É–¥–µ–º –¥–µ—Ä–≥–∞—Ç—å –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –Ω–∞ –¥–∏—Å–∫–µ
    )
    worker.start(poll_interval=settings.WORKER_POLL_INTERVAL, max_workers=settings.WORKER_MAX_CONCURRENT_FILES)

