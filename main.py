"""
ALPACA RAG - –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
"""
import os
from dataclasses import dataclass
from time import sleep
from typing import Dict, List, Tuple
import warnings

os.environ.setdefault("PYTHONWARNINGS", "ignore::UserWarning")
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic_settings.main")

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ Prefect –î–û –∏–º–ø–æ—Ä—Ç–∞
os.environ["PREFECT_LOGGING_LEVEL"] = "WARNING"
os.environ["PREFECT_LOGGING_TO_API_ENABLED"] = "false"

from datetime import timedelta
from prefect import flow, serve, task


@dataclass(frozen=True)
class FileID:
    """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ñ–∞–π–ª–∞ (hash + path)"""
    hash: str
    path: str
from utils.logging import setup_logging, get_logger
from utils.process_lock import ProcessLock
from app.file_watcher import FileWatcherService
from app.flows.file_status_processor import FileStatusProcessorService
from settings import settings
from database import Database

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
setup_logging()
logger = get_logger(__name__)

# –°–µ—Ä–≤–∏—Å—ã
file_watcher = FileWatcherService(
    database_url=settings.DATABASE_URL,
    monitored_path=settings.MONITORED_PATH,
    allowed_extensions=settings.ALLOWED_EXTENSIONS.split(','),
    file_min_size=settings.FILE_MIN_SIZE,
    file_max_size=settings.FILE_MAX_SIZE,
    excluded_dirs=settings.EXCLUDED_DIRS.split(','),
    excluded_patterns=settings.EXCLUDED_PATTERNS.split(',')
)

# file_processor = FileStatusProcessorService(
#     database_url=settings.DATABASE_URL,
#     webhook_url=settings.N8N_WEBHOOK_URL,
#     max_heavy_workflows=settings.MAX_HEAVY_WORKFLOWS
# )

db = Database(settings.DATABASE_URL)


@flow(name="file_watcher_flow")
def file_watcher_flow():
    """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤"""
    result = file_watcher.scan_and_sync()
    
    return result


@task(name="process_deleted_file", retries=2, persist_result=True)
def task_process_deleted_file(
    db: Database, file_id: FileID) -> FileID:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–∞"""
    try:
        chunks_deleted = db.delete_chunks_by_hash(file_id.hash)
        db.delete_file_by_hash(file_id.hash)
        logger.info(f"Deleted {file_id.path} and {chunks_deleted} chunks")
    except Exception as e:
        logger.error(f"ERROR when trying to delete a file {file_id.path}: {e}")
        return None
    return file_id

@task(name="process_added_files", retries=2, persist_result=True)
def task_process_added_files(
    db: Database,
    webhook_url: str,
    files: List[Tuple[str, str, int]],
    slots_available: int
) -> Dict[str, int]:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ added —Ñ–∞–π–ª–æ–≤"""
    stats = {'processed': 0, 'skipped': 0}
    
    for file_path, file_hash, file_size in files:
        if slots_available > 0:
            try:
                logger.info(f"‚ûï Processing added: {file_path}")
                db.call_webhook(webhook_url, file_path, file_hash)
                db.mark_as_processed(file_hash)
                stats['processed'] += 1
                slots_available -= 1
            except Exception as e:
                logger.error(f"‚ùå Failed to process added file {file_path}: {e}")
                db.mark_as_error(file_hash)
        else:
            logger.info(f"‚è∏Ô∏è  Workflow limit reached, skipping remaining added files")
            stats['skipped'] = len(files) - stats['processed']
            break
    
    return stats


@task(name="process_updated_files", retries=2, persist_result=True)
def task_process_updated_files(
    db: Database, file_path, file_hash: str) -> bool:   
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ updated —Ñ–∞–π–ª–æ–≤"""

    
    for file_path, file_hash, file_size in files:
        if slots_available > 0:
            try:
                logger.info(f"üîÑ Processing updated: {file_path}")
                chunks_deleted = db.delete_chunks_by_path(file_path)
                logger.info(f"üóëÔ∏è  Deleted {chunks_deleted} old chunks")
                db.call_webhook(webhook_url, file_path, file_hash)
                db.mark_as_processed(file_hash)
                stats['processed'] += 1
                slots_available -= 1
            except Exception as e:
                logger.error(f"‚ùå Failed to process updated file {file_path}: {e}")
                db.mark_as_error(file_hash)
        else:
            logger.info(f"‚è∏Ô∏è  Workflow limit reached, skipping remaining updated files")
            stats['skipped'] = len(files) - stats['processed']
            break
    
    return stats


@flow(name="parsing_flow")
def parsing_flow(file_id: FileID) -> str:
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ç–µ–∫—Å—Ç
    
    Args:
        file_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ñ–∞–π–ª–∞ (hash + path)
    
    Returns:
        str: –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    logger.info(f"üîç Parsing file: {file_id.path} (hash: {file_id.hash[:8]}...)")
    
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –≤—ã–∑–æ–≤ –ø–∞—Ä—Å–µ—Ä–∞
    # parsed_text = parser_service.parse(file_id.path)    
    sleep(2 + os.urandom(1)[0] / 255 * 3)  # –°–∏–º—É–ª—è—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ 2-5 —Å–µ–∫
    return ""


@flow(name="ingest_files_flow")
def ingest_files_flow():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤ (added/updated ‚Üí ingestion, deleted ‚Üí cleanup)"""
    pending_files = db.get_pending_files()
    total_pending = sum(len(files) for files in pending_files.values())
    logger.info(f"üìã Found {total_pending} pending files (deleted:{len(pending_files['deleted'])}, updated:{len(pending_files['updated'])}, added:{len(pending_files['added'])})")

    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –µ—Å—Ç—å –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ deleted pending-—Ñ–∞–π–ª—ã
    for file_id in pending_files['deleted']:
        task_process_deleted_file(db, file_id)
        
    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –µ—Å—Ç—å –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ deleted pending-—Ñ–∞–π–ª—ã
    for file_id in pending_files['updated']:
        task_process_deleted_file(db, file_id)
        
        
        
        if pending_files['updated'] or pending_files['added']:
            task_process_updated_files(
                db,
                settings.N8N_WEBHOOK_URL,
                pending_files['updated'],
                settings.MAX_HEAVY_WORKFLOWS
            )
            break  # –í—Ä–µ–º–µ–Ω–Ω–æ –ø—Ä–µ—Ä—ã–≤–∞–µ–º —Ü–∏–∫–ª, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–∏—Å–Ω—É—Ç—å
    result = pending_files
    return result
        
        
if __name__ == "__main__":
    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–∫–∞–∫ HTTP —Å–µ—Ä–≤–µ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—Ç)
    process_lock = ProcessLock('/tmp/alpaca_rag.pid')
    process_lock.acquire()
    # process_lock.setup_handlers()  # –û—Ç–∫–ª—é—á–µ–Ω–æ: –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å Prefect Runner SIGTERM
    
    try:
        logger.info("Starting ALPACA RAG system...")
        
        # –°–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–æ–≤ processed —É —Ñ–∞–π–ª–æ–≤ –≤ –±–∞–∑–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        reset_count = file_watcher.reset_processed_statuses()
            
        # –ó–∞–ø—É—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö flows —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        serve(
            file_watcher_flow.to_deployment(
                name="file-watcher",
                interval=timedelta(seconds=settings.SCAN_MONITORED_FOLDER_INTERVAL),
                description="–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤",
                concurrency_limit=1
            ),
            ingest_files_flow.to_deployment(
                name="ingest_files_flow",
                interval=timedelta(seconds=settings.PROCESS_FILE_CHANGES_INTERVAL),
                description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤",
                concurrency_limit=1
            )
        )
    except KeyboardInterrupt:
        logger.info("Shutting down gracefully...")
    finally:
        process_lock.release()
