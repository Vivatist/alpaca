"""
ALPACA RAG - –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
"""
import os
import warnings

os.environ.setdefault("PYTHONWARNINGS", "ignore::UserWarning")
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic_settings.main")

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ Prefect –î–û –∏–º–ø–æ—Ä—Ç–∞
os.environ["PREFECT_LOGGING_LEVEL"] = "WARNING"
os.environ["PREFECT_LOGGING_TO_API_ENABLED"] = "false"

from datetime import timedelta
from typing import Dict, List, Tuple
from prefect import flow, serve, task
from prefect.artifacts import create_table_artifact
from app.utils.logging import setup_logging, get_logger
from app.utils.process_lock import ProcessLock
from app.file_watcher import FileWatcherService
from app.flows.file_status_processor import FileStatusProcessorService
from settings import settings
import time

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
setup_logging()
logger = get_logger(__name__)

# –°–µ—Ä–≤–∏—Å—ã
file_watcher = FileWatcherService(
    database_url=settings.DATABASE_URL,
    monitored_path=settings.MONITORED_PATH,
    allowed_extensions=settings.ALLOWED_EXTENSIONS.split(','),
    file_min_size=settings.FILE_MIN_SIZE,
    file_max_size=settings.FILE_MAX_SIZE,
    excluded_dirs=settings.EXCLUDED_DIRS.split(','),
    excluded_patterns=settings.EXCLUDED_PATTERNS.split(',')
)

file_processor = FileStatusProcessorService(
    database_url=settings.DATABASE_URL,
    webhook_url=settings.N8N_WEBHOOK_URL,
    max_heavy_workflows=settings.MAX_HEAVY_WORKFLOWS
)


# === FILE WATCHER TASKS ===

@task(name="scan_disk", retries=3, persist_result=True)
def task_scan_disk() -> list:
    """Task: —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞"""
    return file_watcher.scan()


@task(name="sync_files_to_db", retries=3, persist_result=True)
def task_sync_files(files: list) -> dict:
    """Task: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ —Å –ë–î –ø–æ —Ö–µ—à–∞–º"""
    return file_watcher.sync_by_hash(files)


@task(name="sync_vector_status", retries=3, persist_result=True)
def task_sync_status() -> dict:
    """Task: —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–æ–≤ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î"""
    return file_watcher.sync_status()


@task(name="reset_processed_statuses", persist_result=True)
def task_reset_processed() -> int:
    """Task: —Å–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–æ–≤ 'processed' –Ω–∞ 'ok'"""
    return file_watcher.reset_processed_statuses()


@flow(name="file_watcher_flow")
def file_watcher_flow():
    """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤"""
    start_time = time.time()
    
    try:
        # –ö–∞–∂–¥—ã–π —à–∞–≥ - –æ—Ç–¥–µ–ª—å–Ω–∞—è task —Å retry –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
        files = task_scan_disk()
        file_sync = task_sync_files(files)
        status_sync = task_sync_status()
        
        duration = time.time() - start_time
        logger.info(
            f"disc[total:{len(files)}, "
            f"+{file_sync['added']}, "
            f"~{file_sync['updated']}, "
            f"-{file_sync['deleted']}]  "
            f"base[ok:{status_sync['ok']}, "
            f"a:{status_sync['added']}, "
            f"u:{status_sync['updated']}] "
            f"in {duration:.2f}s"
        )
        
        # –°–æ–∑–¥–∞—ë–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        create_table_artifact(
            key="scan-summary",
            table=[
                {"Metric": "Files on disk", "Value": len(files)},
                {"Metric": "Added", "Value": file_sync['added']},
                {"Metric": "Updated", "Value": file_sync['updated']},
                {"Metric": "Deleted", "Value": file_sync['deleted']},
                {"Metric": "Unchanged", "Value": file_sync['unchanged']},
                {"Metric": "Status OK", "Value": status_sync['ok']},
                {"Metric": "Status Added", "Value": status_sync['added']},
                {"Metric": "Status Updated", "Value": status_sync['updated']},
                {"Metric": "Duration (s)", "Value": f"{duration:.2f}"},
            ],
            description="File Watcher Scan Summary"
        )
        
        return {
            'success': True,
            'disk_files': len(files),
            'file_sync': file_sync,
            'status_sync': status_sync,
            'duration': duration
        }
        
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"‚ùå Scan failed after {duration:.2f}s: {e}", exc_info=True)
        raise


# === FILE STATUS PROCESSOR TASKS ===

@task(name="get_pending_files", retries=2, persist_result=True)
def task_get_pending_files() -> Dict[str, List[Tuple]]:
    """Task: –ø–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    return file_processor.get_pending_files()


@task(name="get_processed_count", retries=2, persist_result=True)
def task_get_processed_count() -> int:
    """Task: –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ"""
    return file_processor.get_processed_count()


@task(name="call_ingestion_webhook", retries=3, persist_result=True)
def task_call_webhook(file_path: str, file_hash: str) -> bool:
    """Task: –≤—ã–∑–æ–≤ webhook –¥–ª—è ingestion"""
    return file_processor.call_webhook(file_path, file_hash)


@task(name="delete_chunks_by_path", retries=2, persist_result=True)
def task_delete_chunks_by_path(file_path: str) -> int:
    """Task: —É–¥–∞–ª–µ–Ω–∏–µ chunks –ø–æ –ø—É—Ç–∏"""
    return file_processor.delete_chunks_by_path(file_path)


@task(name="delete_chunks_by_hash", retries=2, persist_result=True)
def task_delete_chunks_by_hash(file_hash: str) -> int:
    """Task: —É–¥–∞–ª–µ–Ω–∏–µ chunks –ø–æ —Ö—ç—à—É"""
    return file_processor.delete_chunks_by_hash(file_hash)


@task(name="mark_as_processed", persist_result=True)
def task_mark_as_processed(file_hash: str) -> bool:
    """Task: –ø–æ–º–µ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∫–∞–∫ processed"""
    return file_processor.mark_as_processed(file_hash)


@task(name="mark_as_error", persist_result=True)
def task_mark_as_error(file_hash: str) -> bool:
    """Task: –ø–æ–º–µ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∫–∞–∫ error"""
    return file_processor.mark_as_error(file_hash)


@task(name="delete_file_by_hash", persist_result=True)
def task_delete_file(file_hash: str) -> bool:
    """Task: —É–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞"""
    return file_processor.delete_file(file_hash)


@task(name="process_added_files", retries=2, persist_result=True)
def task_process_added_files(files: List[Tuple[str, str, int]], slots_available: int) -> Dict[str, int]:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ added —Ñ–∞–π–ª–æ–≤"""
    stats = {'processed': 0, 'skipped': 0}
    
    for file_path, file_hash, file_size in files:
        if slots_available > 0:
            try:
                logger.info(f"‚ûï Processing added: {file_path}")
                task_call_webhook(file_path, file_hash)
                task_mark_as_processed(file_hash)
                stats['processed'] += 1
                slots_available -= 1
            except Exception as e:
                logger.error(f"‚ùå Failed to process added file {file_path}: {e}")
                task_mark_as_error(file_hash)
        else:
            logger.info(f"‚è∏Ô∏è  Workflow limit reached, skipping remaining added files")
            stats['skipped'] = len(files) - stats['processed']
            break
    
    return stats


@task(name="process_updated_files", retries=2, persist_result=True)
def task_process_updated_files(files: List[Tuple[str, str, int]], slots_available: int) -> Dict[str, int]:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ updated —Ñ–∞–π–ª–æ–≤"""
    stats = {'processed': 0, 'skipped': 0}
    
    for file_path, file_hash, file_size in files:
        if slots_available > 0:
            try:
                logger.info(f"üîÑ Processing updated: {file_path}")
                chunks_deleted = task_delete_chunks_by_path(file_path)
                logger.info(f"üóëÔ∏è  Deleted {chunks_deleted} old chunks")
                task_call_webhook(file_path, file_hash)
                task_mark_as_processed(file_hash)
                stats['processed'] += 1
                slots_available -= 1
            except Exception as e:
                logger.error(f"‚ùå Failed to process updated file {file_path}: {e}")
                task_mark_as_error(file_hash)
        else:
            logger.info(f"‚è∏Ô∏è  Workflow limit reached, skipping remaining updated files")
            stats['skipped'] = len(files) - stats['processed']
            break
    
    return stats


@task(name="process_deleted_files", retries=2, persist_result=True)
def task_process_deleted_files(files: List[Tuple[str, str, int]]) -> int:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–æ–≤"""
    processed = 0
    
    for file_path, file_hash, file_size in files:
        try:
            logger.info(f"üóëÔ∏è  Processing deleted: {file_path}")
            chunks_deleted = task_delete_chunks_by_hash(file_hash)
            task_delete_file(file_hash)
            logger.info(f"‚úÖ Deleted {chunks_deleted} chunks and file record")
            processed += 1
        except Exception as e:
            logger.error(f"‚ùå Failed to process deleted file {file_path}: {e}")
    
    return processed


@flow(name="file_status_processor_flow")
def file_status_processor_flow():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤ (added/updated ‚Üí ingestion, deleted ‚Üí cleanup)"""
    start_time = time.time()
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª—ã —Ç—Ä–µ–±—É—é—â–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        pending = task_get_pending_files()
        
        total_pending = sum(len(files) for files in pending.values())
        
        if total_pending == 0:
            logger.info("üì≠ No pending files")
            create_table_artifact(
                key="processing-summary",
                table=[
                    {"Metric": "Total processed", "Value": 0},
                    {"Metric": "Added (ingested)", "Value": 0},
                    {"Metric": "Updated (reingested)", "Value": 0},
                    {"Metric": "Deleted (cleaned)", "Value": 0},
                    {"Metric": "Skipped (capacity)", "Value": 0},
                    {"Metric": "Duration (s)", "Value": f"{time.time() - start_time:.2f}"},
                ],
                description="File Status Processor Summary"
            )
            return {
                'success': True,
                'processed': 0,
                'added': 0,
                'updated': 0,
                'deleted': 0,
                'skipped': 0,
                'duration': time.time() - start_time
            }
        
        logger.info(f"üìã Found {total_pending} pending files (added:{len(pending['added'])}, updated:{len(pending['updated'])}, deleted:{len(pending['deleted'])})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–ª–æ—Ç—ã
        current_processed = task_get_processed_count()
        slots_available = file_processor.max_heavy_workflows - current_processed
        logger.info(f"üìä Workflow capacity: {slots_available}/{file_processor.max_heavy_workflows} slots available")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º added —Ñ–∞–π–ª—ã
        added_stats = task_process_added_files(pending['added'], slots_available)
        slots_available -= added_stats['processed']
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º updated —Ñ–∞–π–ª—ã
        updated_stats = task_process_updated_files(pending['updated'], slots_available)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º deleted —Ñ–∞–π–ª—ã (–Ω–µ –∑–∞–Ω–∏–º–∞—é—Ç —Å–ª–æ—Ç—ã)
        deleted_count = task_process_deleted_files(pending['deleted'])
        
        duration = time.time() - start_time
        total_processed = added_stats['processed'] + updated_stats['processed'] + deleted_count
        total_skipped = added_stats['skipped'] + updated_stats['skipped']
        
        logger.info(
            f"‚úÖ Processed {total_processed} files "
            f"(+{added_stats['processed']}, ~{updated_stats['processed']}, -{deleted_count}, "
            f"skipped:{total_skipped}) in {duration:.2f}s"
        )
        
        # –°–æ–∑–¥–∞—ë–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        create_table_artifact(
            key="processing-summary",
            table=[
                {"Metric": "Total processed", "Value": total_processed},
                {"Metric": "Added (ingested)", "Value": added_stats['processed']},
                {"Metric": "Updated (reingested)", "Value": updated_stats['processed']},
                {"Metric": "Deleted (cleaned)", "Value": deleted_count},
                {"Metric": "Skipped (capacity)", "Value": total_skipped},
                {"Metric": "Duration (s)", "Value": f"{duration:.2f}"},
            ],
            description="File Status Processor Summary"
        )
        
        return {
            'success': True,
            'processed': total_processed,
            'added': added_stats['processed'],
            'updated': updated_stats['processed'],
            'deleted': deleted_count,
            'skipped': total_skipped,
            'duration': duration
        }
        
    except Exception as e:
        duration = time.time() - start_time
        logger.error(f"‚ùå Processing failed after {duration:.2f}s: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–∫–∞–∫ HTTP —Å–µ—Ä–≤–µ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—Ç)
    process_lock = ProcessLock('/tmp/alpaca_rag.pid')
    process_lock.acquire()
    process_lock.setup_handlers()
    
    try:
        logger.info("Starting ALPACA RAG system...")
        logger.info(f"Monitored folder: {settings.MONITORED_PATH}")
        logger.info(f"File watcher interval: {settings.SCAN_MONITORED_FOLDER_INTERVAL}s")
        logger.info(f"Status processor interval: {settings.PROCESS_FILE_CHANGES_INTERVAL}s")
        logger.info(f"Max heavy workflows: {settings.MAX_HEAVY_WORKFLOWS}")
        
        # –°–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–æ–≤ processed —É —Ñ–∞–π–ª–æ–≤ –≤ –±–∞–∑–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        reset_count = task_reset_processed()
        
        # –ó–∞–ø—É—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö flows
        serve(
            file_watcher_flow.to_deployment(
                name="file-watcher",
                interval=timedelta(seconds=settings.SCAN_MONITORED_FOLDER_INTERVAL),
                description="–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤"
            ),
            file_status_processor_flow.to_deployment(
                name="file-status-processor",
                interval=timedelta(seconds=settings.PROCESS_FILE_CHANGES_INTERVAL),
                description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤"
            )
        )
    except KeyboardInterrupt:
        logger.info("Shutting down gracefully...")
    finally:
        process_lock.release()
