"""
Worker - –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞, —á–∞–Ω–∫–∏–Ω–≥–∞, —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
"""
import os
from typing import Dict, Any
from threading import Semaphore

from app.parsers.word.parser_word import parser_word_old_task
from app.chunkers.custom_chunker import chunking
from app.embedders.custom_embedder import embedding
from utils.logging import setup_logging, get_logger
from utils.worker import Worker
from settings import settings
from utils.database import PostgreDataBase
from utils.file_manager import File
from tests.runner import run_tests_on_startup

setup_logging()
logger = get_logger("alpaca.worker")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
db = PostgreDataBase(settings.DATABASE_URL)
FILEWATCHER_API = os.getenv("FILEWATCHER_API_URL", "http://localhost:8081")

# –°–µ–º–∞—Ñ–æ—Ä—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (–∏–∑ settings)
PARSE_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_PARSING)
EMBED_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_EMBEDDING)
LLM_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_LLM)


def process_deleted_file(file: File) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–∞ - —É–¥–∞–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏ –∑–∞–ø–∏—Å–∏
    
    Args:
        file: –û–±—ä–µ–∫—Ç File —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–µ
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        chunks_deleted = db.delete_chunks_by_hash(file.hash)
        db.delete_file_by_hash(file.hash)
        logger.info(f"ü™ì Deleted {file.path} and {chunks_deleted} chunks")
        return True
    except Exception as e:
        logger.error(f"Error deleting file {file.path}: {e}")
        return False


def ingest_pipeline(file: File) -> bool:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: –ø–∞—Ä—Å–∏–Ω–≥ ‚Üí —á–∞–Ω–∫–∏–Ω–≥ ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥
    
    Args:
        file: –û–±—ä–µ–∫—Ç File —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–µ
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
    """
    logger.info(f"üçé Start ingest pipeline: {file.path} (hash: {file.hash[:8]}...)")
    
    try:
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
        if file.path.lower().endswith('.docx'):
            logger.info(f"üìñ Parsing file: {file.path}")
            with PARSE_SEMAPHORE:
                raw_text = parser_word_old_task({'hash': file.hash, 'path': file.path})
            logger.info(f"‚úÖ Parsed: {len(raw_text) if raw_text else 0} chars")
        else:
            logger.error(f"Unsupported file type: {file.path}")
            db.mark_as_error(file.hash)
            return False

        if not raw_text or not raw_text.strip():
            logger.error(f"Empty parsed text for {file.path}")
            db.mark_as_error(file.hash)
            return False
        
        # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ temp_parsed
        temp_dir = "/home/alpaca/tmp_md"
        temp_file_path = os.path.join(temp_dir, f"{file.path}.md")
        os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)
        
        with open(temp_file_path, "w", encoding="utf-8") as f:
            f.write(raw_text)
        
        # 3. –ß–∞–Ω–∫–∏–Ω–≥
        chunks = chunking(file.path, raw_text)
        
        if not chunks:
            logger.warning(f"No chunks created for {file.path}")
            db.mark_as_error(file.hash)
            return False
        
        # 4. –≠–º–±–µ–¥–¥–∏–Ω–≥ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
        with EMBED_SEMAPHORE:
            chunks_count = embedding(db, file.hash, file.path, chunks)
        
        if chunks_count == 0:
            logger.warning(f"No embeddings created for {file.path}")
            db.mark_as_error(file.hash)
            return False
        
        db.mark_as_ok(file.hash)
        logger.info(f"‚úÖ File processed successfully: {file.path} | chunks={chunks_count}")
        return True
        
    except Exception as e:
        import traceback
        logger.error(f"Pipeline failed for {file.path}: {e}")
        logger.error(f"Traceback:\n{traceback.format_exc()}")
        db.mark_as_error(file.hash)
        return False


def process_file(file_info: Dict[str, Any]) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª
    
    Args:
        file_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –∏–∑ filewatcher
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
    """
    # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç File –∏–∑ —Å–ª–æ–≤–∞—Ä—è
    file = File(**file_info)
    
    logger.info(f"Processing file: {file.path} (status={file.status_sync})")
    
    try:
        if file.status_sync == 'deleted':
            # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏, –ø–æ—Ç–æ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ updated –µ—Å–ª–∏ —ç—Ç–æ –±—ã–ª updated
            return process_deleted_file(file)
            
        elif file.status_sync == 'updated':
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏, –∑–∞—Ç–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ
            process_deleted_file(file)
            return ingest_pipeline(file)
            
        elif file.status_sync == 'added':
            # –ù–æ–≤—ã–π —Ñ–∞–π–ª - –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            return ingest_pipeline(file)
            
        else:
            logger.warning(f"Unknown status: {file.status_sync} for {file.path}")
            return False
            
    except Exception as e:
        logger.error(f"‚úó Error processing {file.path}: {e}")
        db.mark_as_error(file.hash)
        return False

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö)
    tests_passed = run_tests_on_startup(settings)

    if not tests_passed:
        exit(1)

    # –°–æ–∑–¥–∞—ë–º worker –∏ –∑–∞–ø—É—Å–∫–∞–µ–º
    worker = Worker(
        database_url=settings.DATABASE_URL,
        filewatcher_api_url=FILEWATCHER_API,
        process_file_func=process_file # –ø–µ—Ä–µ–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫–æ—Ç–æ—Ä—É—é –±—É–¥–µ–º –¥–µ—Ä–≥–∞—Ç—å –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –Ω–∞ –¥–∏—Å–∫–µ
    )
    worker.start(poll_interval=settings.WORKER_POLL_INTERVAL, max_workers=settings.WORKER_MAX_CONCURRENT_FILES)

