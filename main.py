"""
Worker - –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞, —á–∞–Ω–∫–∏–Ω–≥–∞, —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤
"""
import os
from typing import Dict, Any
from threading import Semaphore

from app.parsers.word.parser_word import parser_word_old_task
from app.chunkers.custom_chunker import chunking
from app.embedders.custom_embedder import embedding
from utils.logging import setup_logging, get_logger
from utils.worker import Worker
from settings import settings
from utils.database import PostgreDatabase
from tests.runner import run_tests_on_startup

setup_logging()
logger = get_logger("alpaca.worker")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
db = PostgreDatabase(settings.DATABASE_URL)
FILEWATCHER_API = os.getenv("FILEWATCHER_API_URL", "http://localhost:8081")

# –°–µ–º–∞—Ñ–æ—Ä—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (–∏–∑ settings)
PARSE_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_PARSING)
EMBED_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_EMBEDDING)
LLM_SEMAPHORE = Semaphore(settings.WORKER_MAX_CONCURRENT_LLM)


def process_deleted_file(file_hash: str, file_path: str) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–∞ - —É–¥–∞–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏ –∑–∞–ø–∏—Å–∏
    
    Args:
        file_hash: –•—ç—à —Ñ–∞–π–ª–∞
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        chunks_deleted = db.delete_chunks_by_hash(file_hash)
        db.delete_file_by_hash(file_hash)
        logger.info(f"ü™ì Deleted {file_path} and {chunks_deleted} chunks")
        return True
    except Exception as e:
        logger.error(f"Error deleting file {file_path}: {e}")
        return False


def ingest_pipeline(file_hash: str, file_path: str) -> bool:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: –ø–∞—Ä—Å–∏–Ω–≥ ‚Üí —á–∞–Ω–∫–∏–Ω–≥ ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥
    
    Args:
        file_hash: –•—ç—à —Ñ–∞–π–ª–∞
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
    """
    logger.info(f"üçé Start ingest pipeline: {file_path} (hash: {file_hash[:8]}...)")
    
    try:
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
        if file_path.lower().endswith('.docx'):
            logger.info(f"üìñ Parsing file: {file_path}")
            with PARSE_SEMAPHORE:
                raw_text = parser_word_old_task({'hash': file_hash, 'path': file_path})
            logger.info(f"‚úÖ Parsed: {len(raw_text) if raw_text else 0} chars")
        else:
            logger.error(f"Unsupported file type: {file_path}")
            db.mark_as_error(file_hash)
            return False

        if not raw_text or not raw_text.strip():
            logger.error(f"Empty parsed text for {file_path}")
            db.mark_as_error(file_hash)
            return False
        
        # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ temp_parsed
        temp_dir = "/home/alpaca/tmp_md"
        temp_file_path = os.path.join(temp_dir, f"{file_path}.md")
        os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)
        
        with open(temp_file_path, "w", encoding="utf-8") as f:
            f.write(raw_text)
        
        # 3. –ß–∞–Ω–∫–∏–Ω–≥
        chunks = chunking(file_path, raw_text)
        
        if not chunks:
            logger.warning(f"No chunks created for {file_path}")
            db.mark_as_error(file_hash)
            return False
        
        # 4. –≠–º–±–µ–¥–¥–∏–Ω–≥ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
        with EMBED_SEMAPHORE:
            chunks_count = embedding(db, file_hash, file_path, chunks)
        
        if chunks_count == 0:
            logger.warning(f"No embeddings created for {file_path}")
            db.mark_as_error(file_hash)
            return False
        
        db.mark_as_ok(file_hash)
        logger.info(f"‚úÖ File processed successfully: {file_path} | chunks={chunks_count}")
        return True
        
    except Exception as e:
        import traceback
        logger.error(f"Pipeline failed for {file_path}: {e}")
        logger.error(f"Traceback:\n{traceback.format_exc()}")
        db.mark_as_error(file_hash)
        return False


def process_file(file_info: Dict[str, Any]) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª
    
    Args:
        file_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –∏–∑ filewatcher
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
    """
    file_path = file_info['file_path']
    file_hash = file_info['file_hash']
    status = file_info['status_sync']
    
    logger.info(f"Processing file: {file_path} (status={status})")
    
    try:
        if status == 'deleted':
            # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏, –ø–æ—Ç–æ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ updated –µ—Å–ª–∏ —ç—Ç–æ –±—ã–ª updated
            return process_deleted_file(file_hash, file_path)
            
        elif status == 'updated':
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏, –∑–∞—Ç–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ
            process_deleted_file(file_hash, file_path)
            return ingest_pipeline(file_hash, file_path)
            
        elif status == 'added':
            # –ù–æ–≤—ã–π —Ñ–∞–π–ª - –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            return ingest_pipeline(file_hash, file_path)
            
        else:
            logger.warning(f"Unknown status: {status} for {file_path}")
            return False
            
    except Exception as e:
        logger.error(f"‚úó Error processing {file_path}: {e}")
        db.mark_as_error(file_hash)
        return False

if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö)
    tests_passed = run_tests_on_startup(settings)

    if not tests_passed:
        exit(1)

    # –°–æ–∑–¥–∞—ë–º worker –∏ –∑–∞–ø—É—Å–∫–∞–µ–º
    worker = Worker(
        database_url=settings.DATABASE_URL,
        filewatcher_api_url=FILEWATCHER_API,
        process_file_func=process_file # –ø–µ—Ä–µ–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫–æ—Ç–æ—Ä—É—é –±—É–¥–µ–º –¥–µ—Ä–≥–∞—Ç—å –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –Ω–∞ –¥–∏—Å–∫–µ
    )
    worker.start(poll_interval=settings.WORKER_POLL_INTERVAL, max_workers=settings.WORKER_MAX_CONCURRENT_FILES)

