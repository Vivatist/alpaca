"""
ALPACA RAG - –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
"""
import os
from time import sleep
from typing import Dict, List, Tuple
import warnings

os.environ.setdefault("PYTHONWARNINGS", "ignore::UserWarning")
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic_settings.main")

# –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ Prefect –î–û –∏–º–ø–æ—Ä—Ç–∞
os.environ["PREFECT_LOGGING_LEVEL"] = "WARNING"
os.environ["PREFECT_LOGGING_TO_API_ENABLED"] = "false"

from datetime import timedelta
from prefect import flow, serve, task
from pydantic import BaseModel


class FileID(BaseModel):
    """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ñ–∞–π–ª–∞ (hash + path)"""
    hash: str
    path: str
    
    class Config:
        frozen = True
        
        
from utils.logging import setup_logging, get_logger
from utils.process_lock import ProcessLock
from app.file_watcher import FileWatcherService
from settings import settings
from database import Database

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
setup_logging()
logger = get_logger("alpaca.main")

# –°–µ—Ä–≤–∏—Å—ã
file_watcher = FileWatcherService(
    database_url=settings.DATABASE_URL,
    monitored_path=settings.MONITORED_PATH,
    allowed_extensions=settings.ALLOWED_EXTENSIONS.split(','),
    file_min_size=settings.FILE_MIN_SIZE,
    file_max_size=settings.FILE_MAX_SIZE,
    excluded_dirs=settings.EXCLUDED_DIRS.split(','),
    excluded_patterns=settings.EXCLUDED_PATTERNS.split(',')
)

db = Database(settings.DATABASE_URL)


@flow(name="file_watcher_flow")
def file_watcher_flow():
    """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤"""
    result = file_watcher.scan_and_sync()
    
    return result


@task(name="process_deleted_file", retries=2, persist_result=True)
def task_process_deleted_file(
    db: Database, file_id: FileID) -> FileID:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–∞"""
    try:
        chunks_deleted = db.delete_chunks_by_hash(file_id.hash)
        db.delete_file_by_hash(file_id.hash)
        logger.info(f"Deleted {file_id.path} and {chunks_deleted} chunks")
    except Exception as e:
        logger.error(f"ERROR when trying to delete a file {file_id.path}: {e}")
        return None
    return file_id


@flow(name="parsing_flow")
def parsing_flow(file_id: dict) -> str:
    """Flow: –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ç–µ–∫—Å—Ç"""
    file_id = FileID(**file_id)
    
    try:
        logger.info(f"üìñ Processing parsing: {file_id.path}")
        # parsed_text = parser_service.parse(file_id.path)    
        sleep(3)  # –°–∏–º—É–ª—è—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ 2-5 —Å–µ–∫
        return "--text--"  # TODO: –≤–µ—Ä–Ω—É—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    except Exception as e:
        logger.error(f"Failed to process parsing file {file_id.path}: {e}")
        db.mark_as_error(file_id.hash)
        return ""


@flow(name="ingest_pipeline")
def ingest_pipeline(file_id: dict) -> str:
    """–í—Ö–æ–¥–Ω–∞—è —Ç–æ—á–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""    
    file_id = FileID(**file_id)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º dict –æ–±—Ä–∞—Ç–Ω–æ –≤ FileID
    logger.info(f"üçé Start ingest pipeline: {file_id.path} (hash: {file_id.hash[:8]}...)")
    db.mark_as_processed(file_id.hash)
    
    # 1. –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª –≤ —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç
    raw_text = parsing_flow(file_id.model_dump())
    
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω. –ø–æ–∫–∞ —Ç–æ–ª—å–∫–æ –ø–∞—Ä—Å–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    temp_dir = os.path.join(os.path.dirname(__file__), "temp_parsed")
    temp_file_path = os.path.join(temp_dir, f"{file_id.path}.txt")
    
    # –°–æ–∑–¥–∞—ë–º –≤—Å–µ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)
    
    with open(temp_file_path, "w", encoding="utf-8") as f:
        f.write(raw_text)
    
    db.mark_as_ok(file_id.hash)
    logger.info(f"‚úÖ File processed successfully: {file_id.path}")
    return ""


@flow(name="process_pending_files_flow")
def process_pending_files_flow():
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤ (added/updated ‚Üí ingestion, deleted ‚Üí cleanup)"""
    pending_files = db.get_pending_files()
    total_pending = sum(len(files) for files in pending_files.values())
    logger.info(f"üìã Found {total_pending} pending files (deleted:{len(pending_files['deleted'])}, updated:{len(pending_files['updated'])}, added:{len(pending_files['added'])})")

    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –µ—Å—Ç—å –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ deleted pending-—Ñ–∞–π–ª—ã
    for file_id in pending_files['deleted']:
        task_process_deleted_file(db, file_id)

    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –µ—Å—Ç—å –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ updated pending-—Ñ–∞–π–ª—ã
    for file_id in pending_files['updated']:
        task_process_deleted_file(db, file_id)
        ingest_pipeline(file_id.model_dump())

    # –¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –µ—Å—Ç—å –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ added pending-—Ñ–∞–π–ª—ã
    for file_id in pending_files['added']:
        ingest_pipeline(file_id.model_dump())

    return
        
        
if __name__ == "__main__":
    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ (–∫–∞–∫ HTTP —Å–µ—Ä–≤–µ—Ä –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—Ç)
    process_lock = ProcessLock('/tmp/alpaca_rag.pid')
    process_lock.acquire()
    # process_lock.setup_handlers()  # –û—Ç–∫–ª—é—á–µ–Ω–æ: –∫–æ–Ω—Ñ–ª–∏–∫—Ç —Å Prefect Runner SIGTERM
    
    try:
        logger.info("Starting ALPACA RAG system...")
        
        # –°–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–æ–≤ processed —É —Ñ–∞–π–ª–æ–≤ –≤ –±–∞–∑–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        reset_count = file_watcher.reset_processed_statuses()
            
        # –ó–∞–ø—É—Å–∫ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö flows —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        serve(
            file_watcher_flow.to_deployment(
            name="file-watcher",
            interval=timedelta(seconds=settings.SCAN_MONITORED_FOLDER_INTERVAL),
            description="–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤",
            concurrency_limit=1
            ),
            process_pending_files_flow.to_deployment(
            name="process_pending_files_flow",
            interval=timedelta(seconds=settings.PROCESS_FILE_CHANGES_INTERVAL),
            description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤",
            concurrency_limit=settings.MAX_HEAVY_WORKFLOWS
            )
        )
    except KeyboardInterrupt:
        logger.info("Shutting down gracefully...")
    finally:
        process_lock.release()
