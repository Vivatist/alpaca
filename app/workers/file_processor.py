"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ Prefect flows
–ó–∞–º–µ–Ω–∞ N8N workflow –Ω–∞ Python –∫–æ–¥ —Å Prefect –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–µ–π
"""

from pathlib import Path
from typing import Optional
import logging

from prefect import flow, task
from prefect.tasks import task_input_hash
from datetime import timedelta

from settings import settings
from app.core.parser import parse_document
from app.core.chunker import chunk_text
from app.core.embedder import generate_embeddings
from app.db.connection import get_db

logger = logging.getLogger(__name__)


@task(
    name="Parse Document",
    description="–ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ Unstructured API",
    cache_key_fn=task_input_hash,
    cache_expiration=timedelta(hours=24),
    retries=2,
    retry_delay_seconds=30
)
async def parse_document_task(file_path: str, file_hash: str) -> Optional[str]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç
    
    Args:
        file_path: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        file_hash: SHA256 —Ö—ç—à —Ñ–∞–π–ª–∞
    
    Returns:
        –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ None
    """
    logger.info(f"Parsing: {file_path}")
    
    full_path = settings.MONITORED_PATH / file_path
    parsed_text = await parse_document(full_path, output_format='text')
    
    if not parsed_text or len(parsed_text) < 100:
        logger.error(f"Parsed text too short or empty: {file_path}")
        return None
    
    logger.info(f"Parsed {len(parsed_text)} chars from {file_path}")
    return parsed_text


@task(
    name="Chunk Text",
    description="–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏",
    retries=1
)
def chunk_text_task(text: str) -> list[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
    
    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
    """
    chunks = chunk_text(
        text,
        chunk_size=settings.CHUNK_SIZE,
        chunk_overlap=settings.CHUNK_OVERLAP
    )
    
    logger.info(f"Created {len(chunks)} chunks")
    return chunks


@task(
    name="Generate Embeddings",
    description="–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ Ollama",
    retries=2,
    retry_delay_seconds=60
)
async def generate_embeddings_task(chunks: list[str]) -> list[list[float]]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç embeddings –¥–ª—è —á–∞–Ω–∫–æ–≤
    
    Args:
        chunks: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
    
    Returns:
        –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤ embeddings
    """
    embeddings = await generate_embeddings(
        chunks,
        batch_size=settings.PROCESSING_BATCH_SIZE
    )
    
    logger.info(f"Generated {len(embeddings)} embeddings")
    return embeddings


@task(
    name="Save to Database",
    description="–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –∏ embeddings –≤ –ë–î",
    retries=2
)
async def save_to_database_task(
    file_hash: str,
    file_path: str,
    chunks: list[str],
    embeddings: list[list[float]]
) -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–∞–Ω–∫–∏ –≤ documents —Ç–∞–±–ª–∏—Ü—É
    
    Args:
        file_hash: SHA256 —Ö—ç—à —Ñ–∞–π–ª–∞
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        embeddings: –°–ø–∏—Å–æ–∫ embeddings
    
    Returns:
        True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
    """
    if len(chunks) != len(embeddings):
        logger.error(f"Mismatch: {len(chunks)} chunks vs {len(embeddings)} embeddings")
        return False
    
    async with get_db() as db:
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        await db.execute(
            "DELETE FROM documents WHERE file_hash = $1",
            file_hash
        )
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
        for idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            await db.execute("""
                INSERT INTO documents 
                (file_hash, file_path, chunk_index, chunk_text, embedding)
                VALUES ($1, $2, $3, $4, $5)
            """, file_hash, file_path, idx, chunk, embedding)
        
        logger.info(f"Saved {len(chunks)} chunks to database")
        return True


@task(
    name="Update File Status",
    description="–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å —Ñ–∞–π–ª–∞ –≤ file_state",
    retries=2
)
async def update_file_status_task(file_hash: str, status: str) -> bool:
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç status_sync –≤ file_state
    
    Args:
        file_hash: SHA256 —Ö—ç—à —Ñ–∞–π–ª–∞
        status: –ù–æ–≤—ã–π —Å—Ç–∞—Ç—É—Å ('ok', 'error', etc.)
    
    Returns:
        True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
    """
    async with get_db() as db:
        await db.execute("""
            UPDATE file_state 
            SET status_sync = $1, last_checked = NOW()
            WHERE file_hash = $2
        """, status, file_hash)
    
    logger.info(f"Updated status to '{status}' for hash {file_hash[:8]}...")
    return True


@flow(
    name="Process Document",
    description="–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: parse ‚Üí chunk ‚Üí embed ‚Üí save",
    retries=1,
    retry_delay_seconds=120
)
async def process_document_flow(file_path: str, file_hash: str) -> bool:
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∑–∞–º–µ–Ω–∞ N8N workflow)
    
    –®–∞–≥–∏:
    1. –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Unstructured API
    2. –ß–∞–Ω–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —á–µ—Ä–µ–∑ Ollama
    4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
    5. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
    
    Args:
        file_path: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        file_hash: SHA256 —Ö—ç—à —Ñ–∞–π–ª–∞
    
    Returns:
        True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, False –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
    """
    logger.info(f"üöÄ Processing: {file_path} (hash: {file_hash[:8]}...)")
    
    try:
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        parsed_text = await parse_document_task(file_path, file_hash)
        
        if not parsed_text:
            await update_file_status_task(file_hash, 'error')
            return False
        
        # 2. –ß–∞–Ω–∫–∏—Ä–æ–≤–∞–Ω–∏–µ
        chunks = chunk_text_task(parsed_text)
        
        if not chunks:
            await update_file_status_task(file_hash, 'error')
            return False
        
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings
        embeddings = await generate_embeddings_task(chunks)
        
        if not embeddings or len(embeddings) != len(chunks):
            await update_file_status_task(file_hash, 'error')
            return False
        
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        success = await save_to_database_task(file_hash, file_path, chunks, embeddings)
        
        if not success:
            await update_file_status_task(file_hash, 'error')
            return False
        
        # 5. –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ 'ok'
        await update_file_status_task(file_hash, 'ok')
        
        logger.info(f"‚úÖ Successfully processed {file_path}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Failed to process {file_path}: {e}", exc_info=True)
        
        # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ error
        try:
            await update_file_status_task(file_hash, 'error')
        except Exception as db_error:
            logger.error(f"Failed to update error status: {db_error}")
        
        return False


@flow(
    name="Process Queue",
    description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—á–µ—Ä–µ–¥–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ file_state",
    log_prints=True
)
async def process_queue_flow() -> dict:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—á–µ—Ä–µ–¥–∏ —Ñ–∞–π–ª–æ–≤ (–∑–∞–º–µ–Ω–∞ main-loop)
    
    –õ–æ–≥–∏–∫–∞:
    - –ë–µ—Ä—ë–º —Ñ–∞–π–ª—ã —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'added' –∏–ª–∏ 'updated'
    - –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–µ –±–æ–ª—å—à–µ MAX_CONCURRENT_PROCESSING –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    - –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å 'processed' —Ñ–∞–π–ª—ã
    
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    async with get_db() as db:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö
        current_processing = await db.fetchval("""
            SELECT COUNT(*) FROM file_state 
            WHERE status_sync = 'processed'
        """)
        
        current_processing = current_processing or 0
        slots_available = settings.MAX_CONCURRENT_PROCESSING - current_processing
        
        if slots_available <= 0:
            logger.debug("No available slots for processing")
            return {'processed': 0, 'skipped': 0}
        
        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        files = await db.fetch("""
            SELECT file_path, file_hash, file_size
            FROM file_state
            WHERE status_sync IN ('added', 'updated')
            ORDER BY last_checked ASC
            LIMIT $1
        """, slots_available)
        
        if not files:
            logger.debug("No files to process")
            return {'processed': 0, 'skipped': 0}
        
        logger.info(f"üìã Processing {len(files)} files")
        
        # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ 'processed'
        for file in files:
            await db.execute("""
                UPDATE file_state 
                SET status_sync = 'processed'
                WHERE file_hash = $1
            """, file['file_hash'])
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã —á–µ—Ä–µ–∑ Prefect subflows
        results = []
        for file in files:
            result = await process_document_flow(
                file['file_path'],
                file['file_hash']
            )
            results.append(result)
        
        success_count = sum(1 for r in results if r)
        failed_count = len(results) - success_count
        
        logger.info(
            f"üìä Queue processed: {success_count} success, {failed_count} failed"
        )
        
        return {
            'processed': success_count,
            'failed': failed_count,
            'total': len(results)
        }
