"""Custom chunker Ð´Ð»Ñ Ñ€Ð°Ð·Ð±Ð¸Ð²ÐºÐ¸ Ñ‚ÐµÐºÑÑ‚Ð° Ð½Ð° Ñ‡Ð°Ð½ÐºÐ¸"""
from typing import List
from utils.logging import get_logger
from alpaca.domain.files.models import FileSnapshot

logger = get_logger("alpaca.chunker")


def chunking(file: FileSnapshot) -> List[str]:
    """Ð Ð°Ð·Ð±Ð¸Ð²ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð° Ð½Ð° Ñ‡Ð°Ð½ÐºÐ¸
    
    Args:
        file_path: ÐŸÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ (Ð´Ð»Ñ Ð»Ð¾Ð³Ð¾Ð²)
        text: Ð Ð°ÑÐ¿Ð°Ñ€ÑÐµÐ½Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
        
    Returns:
        List[str]: ÑÐ¿Ð¸ÑÐ¾Ðº Ñ‡Ð°Ð½ÐºÐ¾Ð²
    """
    try:
        logger.info(f"ðŸ”ª Chunking: {file.path}")
        
        chunks = []
        max_chunk_size = 1000  # ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
        paragraphs = (file.raw_text or "").split('\n\n')
        
        current_chunk = ""
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            if len(current_chunk) + len(para) > max_chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = para
            else:
                current_chunk += "\n\n" + para if current_chunk else para
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        logger.info(f"âœ… Created {len(chunks)} chunks for {file.path}")
        return chunks
        
    except Exception as e:
        logger.error(f"Failed to chunk text | file={file.path} error={e}")
        return []
