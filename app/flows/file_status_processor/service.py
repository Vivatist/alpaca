"""
File Status Processor Service - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ñ–∞–π–ª–æ–≤
"""
import time
import requests
from typing import Dict, Any, Tuple, List
from prefect import task
from app.utils.logging import get_logger
from .database import Database


logger = get_logger(__name__)


# Prefect tasks –∫–∞–∫ —Ç–æ–Ω–∫–∏–µ –æ–±—ë—Ä—Ç–∫–∏ –Ω–∞–¥ –º–µ—Ç–æ–¥–∞–º–∏ (–ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å file_watcher)
@task(name="get_pending_files", retries=2, persist_result=True)
def task_get_pending_files(db: Database) -> Dict[str, List[Tuple]]:
    """Task: –ø–æ–ª—É—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    return db.get_pending_files()


@task(name="get_processed_count", retries=2, persist_result=True)
def task_get_processed_count(db: Database) -> int:
    """Task: –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ"""
    return db.get_processed_count()


@task(name="call_ingestion_webhook", retries=3, persist_result=True)
def task_call_webhook(webhook_url: str, file_path: str, file_hash: str) -> bool:
    """Task: –≤—ã–∑–æ–≤ webhook –¥–ª—è ingestion"""
    response = requests.post(webhook_url, json={
        'file_path': file_path,
        'file_hash': file_hash,
        'operation': 'process_document'
    }, timeout=5)
    response.raise_for_status()
    return True


@task(name="delete_chunks_by_path", retries=2, persist_result=True)
def task_delete_chunks_by_path(db: Database, file_path: str) -> int:
    """Task: —É–¥–∞–ª–µ–Ω–∏–µ chunks –ø–æ –ø—É—Ç–∏"""
    return db.delete_chunks_by_path(file_path)


@task(name="delete_chunks_by_hash", retries=2, persist_result=True)
def task_delete_chunks_by_hash(db: Database, file_hash: str) -> int:
    """Task: —É–¥–∞–ª–µ–Ω–∏–µ chunks –ø–æ —Ö—ç—à—É"""
    return db.delete_chunks_by_hash(file_hash)


@task(name="mark_as_processed", persist_result=True)
def task_mark_as_processed(db: Database, file_hash: str) -> bool:
    """Task: –ø–æ–º–µ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∫–∞–∫ processed"""
    return db.mark_as_processed(file_hash)


@task(name="mark_as_error", persist_result=True)
def task_mark_as_error(db: Database, file_hash: str) -> bool:
    """Task: –ø–æ–º–µ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –∫–∞–∫ error"""
    return db.mark_as_error(file_hash)


@task(name="delete_file_by_hash", persist_result=True)
def task_delete_file(db: Database, file_hash: str) -> bool:
    """Task: —É–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞"""
    return db.delete_file_by_hash(file_hash)


@task(name="process_added_files", retries=2, persist_result=True)
def task_process_added_files(
    db: Database, 
    webhook_url: str, 
    files: List[Tuple[str, str, int]], 
    slots_available: int
) -> Dict[str, int]:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ added —Ñ–∞–π–ª–æ–≤"""
    stats = {'processed': 0, 'skipped': 0}
    
    for file_path, file_hash, file_size in files:
        if slots_available > 0:
            try:
                logger.info(f"‚ûï Processing added: {file_path}")
                task_call_webhook(webhook_url, file_path, file_hash)
                task_mark_as_processed(db, file_hash)
                stats['processed'] += 1
                slots_available -= 1
            except Exception as e:
                logger.error(f"‚ùå Failed to process added file {file_path}: {e}")
                task_mark_as_error(db, file_hash)
        else:
            logger.info(f"‚è∏Ô∏è  Workflow limit reached, skipping remaining added files")
            stats['skipped'] = len(files) - stats['processed']
            break
    
    return stats


@task(name="process_updated_files", retries=2, persist_result=True)
def task_process_updated_files(
    db: Database, 
    webhook_url: str, 
    files: List[Tuple[str, str, int]], 
    slots_available: int
) -> Dict[str, int]:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ updated —Ñ–∞–π–ª–æ–≤"""
    stats = {'processed': 0, 'skipped': 0}
    
    for file_path, file_hash, file_size in files:
        if slots_available > 0:
            try:
                logger.info(f"üîÑ Processing updated: {file_path}")
                chunks_deleted = task_delete_chunks_by_path(db, file_path)
                logger.info(f"üóëÔ∏è  Deleted {chunks_deleted} old chunks")
                task_call_webhook(webhook_url, file_path, file_hash)
                task_mark_as_processed(db, file_hash)
                stats['processed'] += 1
                slots_available -= 1
            except Exception as e:
                logger.error(f"‚ùå Failed to process updated file {file_path}: {e}")
                task_mark_as_error(db, file_hash)
        else:
            logger.info(f"‚è∏Ô∏è  Workflow limit reached, skipping remaining updated files")
            stats['skipped'] = len(files) - stats['processed']
            break
    
    return stats


@task(name="process_deleted_files", retries=2, persist_result=True)
def task_process_deleted_files(
    db: Database, 
    files: List[Tuple[str, str, int]]
) -> int:
    """Task: –æ–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–æ–≤"""
    processed = 0
    
    for file_path, file_hash, file_size in files:
        try:
            logger.info(f"üóëÔ∏è  Processing deleted: {file_path}")
            chunks_deleted = task_delete_chunks_by_hash(db, file_hash)
            task_delete_file(db, file_hash)
            logger.info(f"‚úÖ Deleted {chunks_deleted} chunks and file record")
            processed += 1
        except Exception as e:
            logger.error(f"‚ùå Failed to process deleted file {file_path}: {e}")
    
    return processed


class FileStatusProcessorService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å—Ç–∞—Ç—É—Å–æ–≤ —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(
        self,
        database_url: str,
        webhook_url: str,
        max_heavy_workflows: int = 2
    ):
        """
        Args:
            database_url: URL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            webhook_url: URL webhook –¥–ª—è –∑–∞–ø—É—Å–∫–∞ ingestion pipeline
            max_heavy_workflows: –ú–∞–∫—Å–∏–º—É–º —Ç—è–∂—ë–ª—ã—Ö –≤–æ—Ä–∫—Ñ–ª–æ—É –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        """
        self.db = Database(database_url=database_url)
        self.webhook_url = webhook_url
        self.max_heavy_workflows = max_heavy_workflows
    
    def process_changes(self) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ñ–∞–π–ª–æ–≤.
        –í—ã–∑—ã–≤–∞–µ—Ç Prefect tasks –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞.
        
        Returns:
            dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        start_time = time.time()
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª—ã —Ç—Ä–µ–±—É—é—â–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (task)
            pending = task_get_pending_files(self.db)
            
            total_pending = sum(len(files) for files in pending.values())
            
            if total_pending == 0:
                logger.info("üì≠ No pending files")
                return {
                    'success': True,
                    'processed': 0,
                    'added': 0,
                    'updated': 0,
                    'deleted': 0,
                    'skipped': 0,
                    'duration': time.time() - start_time
                }
            
            logger.info(f"üìã Found {total_pending} pending files (added:{len(pending['added'])}, updated:{len(pending['updated'])}, deleted:{len(pending['deleted'])})")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–ª–æ—Ç—ã (task)
            current_processed = task_get_processed_count(self.db)
            slots_available = self.max_heavy_workflows - current_processed
            logger.info(f"üìä Workflow capacity: {slots_available}/{self.max_heavy_workflows} slots available")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º added —Ñ–∞–π–ª—ã (task)
            added_stats = task_process_added_files(
                self.db, 
                self.webhook_url, 
                pending['added'], 
                slots_available
            )
            slots_available -= added_stats['processed']
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º updated —Ñ–∞–π–ª—ã (task)
            updated_stats = task_process_updated_files(
                self.db, 
                self.webhook_url, 
                pending['updated'], 
                slots_available
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º deleted —Ñ–∞–π–ª—ã (task, –Ω–µ –∑–∞–Ω–∏–º–∞—é—Ç —Å–ª–æ—Ç—ã)
            deleted_count = task_process_deleted_files(self.db, pending['deleted'])
            
            duration = time.time() - start_time
            total_processed = added_stats['processed'] + updated_stats['processed'] + deleted_count
            total_skipped = added_stats['skipped'] + updated_stats['skipped']
            
            logger.info(
                f"‚úÖ Processed {total_processed} files "
                f"(+{added_stats['processed']}, ~{updated_stats['processed']}, -{deleted_count}, "
                f"skipped:{total_skipped}) in {duration:.2f}s"
            )
            
            return {
                'success': True,
                'processed': total_processed,
                'added': added_stats['processed'],
                'updated': updated_stats['processed'],
                'deleted': deleted_count,
                'skipped': total_skipped,
                'duration': duration
            }
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"‚ùå Processing failed after {duration:.2f}s: {e}", exc_info=True)
            return {
                'success': False,
                'error': str(e),
                'duration': duration
            }
