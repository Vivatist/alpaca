# –ö–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π Cleaner –≤ ALPACA RAG

## –û–±–∑–æ—Ä

Cleaner ‚Äî —ç—Ç–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞, –∫–æ—Ç–æ—Ä—ã–π **–æ—á–∏—â–∞–µ—Ç/–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç** –º–µ–∂–¥—É –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∏ —á–∞–Ω–∫–∏–Ω–≥–æ–º (–∏–ª–∏ –º–µ–∂–¥—É —á–∞–Ω–∫–∏–Ω–≥–æ–º –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º). 

–¢–µ–∫—É—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω:
```
Parser ‚Üí Chunker ‚Üí Embedder
```

–ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è cleaner:
```
Parser ‚Üí Cleaner ‚Üí Chunker ‚Üí Embedder
```

---

## –®–∞–≥ 1: –°–æ–∑–¥–∞—Ç—å –¥–æ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç (type alias)

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `core/domain/document_processing/cleaners/__init__.py`:

```python
"""
–î–æ–º–µ–Ω–Ω—ã–π —Ç–∏–ø –¥–ª—è –∫–ª–∏–Ω–µ—Ä–æ–≤ (Cleaner).

=== –ù–ê–ó–ù–ê–ß–ï–ù–ò–ï ===
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ç—Ä–∞–∫—Ç –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π, –æ—á–∏—â–∞—é—â–∏—Ö —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
–¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–µ—Ä–µ–¥ —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º –Ω–∞ —á–∞–Ω–∫–∏.

=== –°–ò–ì–ù–ê–¢–£–†–ê ===
    Cleaner = Callable[[FileSnapshot], str]

–ü—Ä–∏–Ω–∏–º–∞–µ—Ç: FileSnapshot —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º raw_text
–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (—Å—Ç—Ä–æ–∫–∞)

=== –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï ===

    from core.domain.document_processing.cleaners import Cleaner
    from core.domain.files import FileSnapshot

    # –¢–∏–ø–∏–∑–∞—Ü–∏—è –∫–ª–∏–Ω–µ—Ä–∞
    def my_cleaner(file: FileSnapshot) -> str:
        text = file.raw_text or ""
        # –£–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = " ".join(text.split())
        return text

    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ use-case
    cleaner: Cleaner = my_cleaner
    cleaned_text = cleaner(file)
"""

from __future__ import annotations

from typing import Callable

from core.domain.files.models import FileSnapshot

# –ö–æ–Ω—Ç—Ä–∞–∫—Ç: –ø—Ä–∏–Ω–∏–º–∞–µ—Ç FileSnapshot, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
Cleaner = Callable[[FileSnapshot], str]

__all__ = ["Cleaner"]
```

---

## –®–∞–≥ 2: –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç—Ä–∞–∫—Ç –∏–∑ –¥–æ–º–µ–Ω–∞

–û–±–Ω–æ–≤–∏—Ç–µ —Ñ–∞–π–ª `core/domain/document_processing/__init__.py`:

```python
"""
–î–æ–º–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
"""

from .parsers import ParserProtocol
from .parsers.registry import ParserRegistry
from .chunkers import Chunker
from .cleaners import Cleaner   # <-- –î–æ–±–∞–≤–∏—Ç—å
from .embedders import Embedder

__all__ = [
    "ParserProtocol",
    "ParserRegistry", 
    "Chunker",
    "Cleaner",     # <-- –î–æ–±–∞–≤–∏—Ç—å
    "Embedder",
]
```

---

## –®–∞–≥ 3: –°–æ–∑–¥–∞—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é cleaner

–°–æ–∑–¥–∞–π—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ —Ñ–∞–π–ª:
```
core/application/document_processing/cleaners/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ text_cleaner.py
```

### `core/application/document_processing/cleaners/__init__.py`:

```python
"""–ú–æ–¥—É–ª—å –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞."""

from .text_cleaner import clean_text

__all__ = ["clean_text"]
```

### `core/application/document_processing/cleaners/text_cleaner.py`:

```python
"""–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–ª–∏–Ω–µ—Ä–∞."""
import re
from typing import Optional
from utils.logging import get_logger
from core.domain.files.models import FileSnapshot

logger = get_logger("core.cleaner")


def clean_text(file: FileSnapshot) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞.
    
    Args:
        file: FileSnapshot —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º raw_text
        
    Returns:
        str: –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    text = file.raw_text or ""
    
    if not text:
        logger.warning(f"Empty text for {file.path}")
        return ""
    
    try:
        logger.info(f"üßπ Cleaning: {file.path}")
        
        original_len = len(text)
        
        # 1. –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r' +', ' ', text)
        
        # 2. –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ (–±–æ–ª—å—à–µ 2)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # 3. –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫
        text = '\n'.join(line.strip() for line in text.split('\n'))
        
        # 4. –£–¥–∞–ª—è–µ–º —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã (–∫—Ä–æ–º–µ \n –∏ \t)
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
        
        # 5. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º Unicode –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[\u00a0\u2000-\u200b\u202f\u205f\u3000]', ' ', text)
        
        # 6. –§–∏–Ω–∞–ª—å–Ω—ã–π strip
        text = text.strip()
        
        cleaned_len = len(text)
        reduction = ((original_len - cleaned_len) / original_len * 100) if original_len > 0 else 0
        
        logger.info(f"‚úÖ Cleaned: {file.path} | {original_len} ‚Üí {cleaned_len} chars ({reduction:.1f}% reduced)")
        
        return text
        
    except Exception as e:
        logger.error(f"‚ùå Cleaning failed | file={file.path} error={e}")
        return file.raw_text or ""  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
```

---

## –®–∞–≥ 4: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤ use-case IngestDocument

–û–±–Ω–æ–≤–∏—Ç–µ `core/application/processing/use_cases.py`:

```python
from __future__ import annotations

from dataclasses import dataclass, field
from threading import Semaphore
from typing import Dict, Any, Optional
import os

from utils.logging import get_logger
from core.domain.files.repository import FileRepository
from core.domain.files.models import FileSnapshot
from core.domain.document_processing import ParserRegistry, Chunker, Embedder, Cleaner  # <-- –î–æ–±–∞–≤–∏—Ç—å Cleaner


@dataclass
class IngestDocument:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (parse ‚Üí clean ‚Üí chunk ‚Üí embed)."""

    repository: FileRepository
    parser_registry: ParserRegistry
    chunker: Chunker
    embedder: Embedder
    parse_semaphore: Semaphore
    embed_semaphore: Semaphore
    cleaner: Optional[Cleaner] = None    # <-- –î–æ–±–∞–≤–∏—Ç—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    temp_dir: str = "/home/alpaca/tmp_md"
    logger_name: str = field(default="core.ingest")

    def __post_init__(self):
        self.logger = get_logger(self.logger_name)

    def __call__(self, file: FileSnapshot) -> bool:
        self.logger.info(f"üçé Start ingest pipeline: {file.path}")
        try:
            # 1. Parse
            parser = self.parser_registry.get_parser(file.path)
            if parser is None:
                self.logger.error(f"Unsupported file type: {file.path}")
                self.repository.mark_as_error(file.hash)
                return False

            with self.parse_semaphore:
                file.raw_text = parser.parse(file)
                self.repository.set_raw_text(file.hash, file.raw_text)

            self.logger.info(f"‚úÖ Parsed: {len(file.raw_text) if file.raw_text else 0} chars")

            # 2. Clean (–µ—Å–ª–∏ cleaner –∑–∞–¥–∞–Ω)         # <-- –ù–æ–≤—ã–π —à–∞–≥
            if self.cleaner is not None:
                file.raw_text = self.cleaner(file)
                self.logger.info(f"‚úÖ Cleaned: {len(file.raw_text) if file.raw_text else 0} chars")

            # 3. Save to disk for debugging
            self._save_to_disk(file)

            # 4. Chunk
            chunks = self.chunker(file)
            if not chunks:
                self.logger.warning(f"No chunks created for {file.path}")
                self.repository.mark_as_error(file.hash)
                return False

            # 5. Embed
            with self.embed_semaphore:
                chunks_count = self.embedder(self.repository, file, chunks)

            # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
```

---

## –®–∞–≥ 5: –ü–æ–¥–∫–ª—é—á–∏—Ç—å –≤ bootstrap

–û–±–Ω–æ–≤–∏—Ç–µ `core/application/bootstrap.py`:

```python
from core.application.document_processing.chunkers import chunk_document as default_chunker
from core.application.document_processing.cleaners import clean_text  # <-- –î–æ–±–∞–≤–∏—Ç—å
from core.application.document_processing.embedders import custom_embedding, langchain_embedding


def build_worker_application(app_settings: Settings = settings) -> WorkerApplication:
    """–°–æ–±–∏—Ä–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–π worker."""
    
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...
    
    # 3. Chunker
    chunker = default_chunker
    
    # 3.5 Cleaner (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)                    # <-- –î–æ–±–∞–≤–∏—Ç—å
    cleaner = clean_text if app_settings.ENABLE_CLEANER else None
    
    # 4. Embedder
    if app_settings.EMBEDDER_BACKEND == "langchain":
        embedder = langchain_embedding
    else:
        embedder = custom_embedding
    
    # 5. Ingest pipeline
    ingest = IngestDocument(
        repository=repository,
        parser_registry=parsers,
        chunker=chunker,
        cleaner=cleaner,           # <-- –î–æ–±–∞–≤–∏—Ç—å
        embedder=embedder,
        parse_semaphore=Semaphore(app_settings.WORKER_MAX_CONCURRENT_PARSING),
        embed_semaphore=Semaphore(app_settings.WORKER_MAX_CONCURRENT_EMBEDDING),
    )
    
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
```

---

## –®–∞–≥ 6: –î–æ–±–∞–≤–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫—É –≤ settings.py

```python
# settings.py

class Settings(BaseSettings):
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ...
    
    # Cleaner
    ENABLE_CLEANER: bool = True  # –í–∫–ª—é—á–∏—Ç—å –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞
```

---

## –®–∞–≥ 7: –ù–∞–ø–∏—Å–∞—Ç—å —Ç–µ—Å—Ç—ã

–°–æ–∑–¥–∞–π—Ç–µ `tests/unit/test_cleaner.py`:

```python
"""–¢–µ—Å—Ç—ã –¥–ª—è text_cleaner."""
import pytest
from core.domain.files.models import FileSnapshot
from core.application.document_processing.cleaners import clean_text


@pytest.fixture
def file_snapshot():
    """–§–∏–∫—Å—Ç—É—Ä–∞ FileSnapshot."""
    return FileSnapshot(
        hash="abc123",
        path="test.docx",
        size=1000,
        mtime=1234567890.0,
        status_sync="added",
        raw_text=None,
    )


def test_clean_text_removes_extra_spaces(file_snapshot):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤."""
    file_snapshot.raw_text = "Hello    world   test"
    result = clean_text(file_snapshot)
    assert result == "Hello world test"


def test_clean_text_removes_extra_newlines(file_snapshot):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–µ—Ä–µ–Ω–æ—Å–æ–≤."""
    file_snapshot.raw_text = "Line1\n\n\n\n\nLine2"
    result = clean_text(file_snapshot)
    assert result == "Line1\n\nLine2"


def test_clean_text_handles_empty(file_snapshot):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    file_snapshot.raw_text = ""
    result = clean_text(file_snapshot)
    assert result == ""


def test_clean_text_handles_none(file_snapshot):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É None."""
    file_snapshot.raw_text = None
    result = clean_text(file_snapshot)
    assert result == ""


def test_clean_text_strips_lines(file_snapshot):
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫."""
    file_snapshot.raw_text = "  Line1  \n  Line2  "
    result = clean_text(file_snapshot)
    assert result == "Line1\nLine2"
```

–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤:
```bash
cd ~/alpaca && source venv/bin/activate
pytest tests/unit/test_cleaner.py -v
```

---

## –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
core/
‚îú‚îÄ‚îÄ domain/
‚îÇ   ‚îî‚îÄ‚îÄ document_processing/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py          # –≠–∫—Å–ø–æ—Ä—Ç Cleaner
‚îÇ       ‚îú‚îÄ‚îÄ cleaners/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py      # Cleaner type alias
‚îÇ       ‚îú‚îÄ‚îÄ chunkers/
‚îÇ       ‚îú‚îÄ‚îÄ embedders/
‚îÇ       ‚îî‚îÄ‚îÄ parsers/
‚îÇ
‚îú‚îÄ‚îÄ application/
‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.py              # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ cleaner
‚îÇ   ‚îú‚îÄ‚îÄ document_processing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaners/             # –ù–û–í–ê–Ø –î–ò–†–ï–ö–¢–û–†–ò–Ø
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_cleaner.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunkers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedders/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parsers/
‚îÇ   ‚îî‚îÄ‚îÄ processing/
‚îÇ       ‚îî‚îÄ‚îÄ use_cases.py          # IngestDocument —Å cleaner
‚îÇ
settings.py                       # ENABLE_CLEANER
tests/
‚îî‚îÄ‚îÄ unit/
    ‚îî‚îÄ‚îÄ test_cleaner.py           # –¢–µ—Å—Ç—ã
```

---

## –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ—á–∏—Å—Ç–∫–∏

### –ù–µ—Å–∫–æ–ª—å–∫–æ cleaner'–æ–≤ –≤ —Ü–µ–ø–æ—á–∫–µ

–ï—Å–ª–∏ –Ω—É–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—á–∏—Å—Ç–æ–∫, —Å–æ–∑–¥–∞–π—Ç–µ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π cleaner:

```python
# core/application/document_processing/cleaners/composite.py

from typing import List
from core.domain.document_processing.cleaners import Cleaner
from core.domain.files.models import FileSnapshot


def compose_cleaners(*cleaners: Cleaner) -> Cleaner:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–∏–Ω–µ—Ä–æ–≤ –≤ —Ü–µ–ø–æ—á–∫—É."""
    
    def composite(file: FileSnapshot) -> str:
        text = file.raw_text
        for cleaner in cleaners:
            file.raw_text = text
            text = cleaner(file)
        return text
    
    return composite


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
from .text_cleaner import clean_text
from .html_cleaner import clean_html

full_cleaner = compose_cleaners(clean_html, clean_text)
```

### Cleaner –ø–æ —Ç–∏–ø—É —Ñ–∞–π–ª–∞

```python
# core/application/document_processing/cleaners/registry.py

from typing import Dict, Tuple, Optional
from core.domain.document_processing.cleaners import Cleaner


class CleanerRegistry:
    """–†–µ–µ—Å—Ç—Ä –∫–ª–∏–Ω–µ—Ä–æ–≤ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º —Ñ–∞–π–ª–æ–≤."""
    
    def __init__(self, cleaners: Dict[Tuple[str, ...], Cleaner], default: Cleaner):
        self._cleaners = cleaners
        self._default = default
    
    def get_cleaner(self, file_path: str) -> Cleaner:
        lower_path = file_path.lower()
        for extensions, cleaner in self._cleaners.items():
            if lower_path.endswith(extensions):
                return cleaner
        return self._default
```

---

## –ß–µ–∫-–ª–∏—Å—Ç

- [ ] –°–æ–∑–¥–∞—Ç—å `core/domain/document_processing/cleaners/__init__.py` —Å type alias
- [ ] –î–æ–±–∞–≤–∏—Ç—å —ç–∫—Å–ø–æ—Ä—Ç –≤ `core/domain/document_processing/__init__.py`
- [ ] –°–æ–∑–¥–∞—Ç—å `core/application/document_processing/cleaners/` —Å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `IngestDocument` ‚Äî –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä `cleaner`
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `bootstrap.py` ‚Äî –ø–æ–¥–∫–ª—é—á–∏—Ç—å cleaner
- [ ] –î–æ–±–∞–≤–∏—Ç—å `ENABLE_CLEANER` –≤ `settings.py`
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å unit-—Ç–µ—Å—Ç—ã
- [ ] –ó–∞–ø—É—Å—Ç–∏—Ç—å `pytest tests/unit/test_cleaner.py -v`
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é: `python main.py`
