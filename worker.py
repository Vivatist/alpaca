"""
–ü—Ä–æ—Å—Ç–æ–π worker –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ –æ—á–µ—Ä–µ–¥–∏ –±–µ–∑ Prefect
"""
import os
import time
import requests
import psycopg2
import psycopg2.extras
from typing import Optional, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Semaphore

from app.parsers.word.parser_word import parser_word_old_task
from app.chunkers.custom_chunker import chunking
from app.embedders.custom_embedder import embedding
from utils.logging import setup_logging, get_logger
from settings import settings
from database import Database
from tests.runner import run_tests_on_startup

setup_logging()
logger = get_logger("alpaca.worker")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
db = Database(settings.DATABASE_URL)
FILEWATCHER_API = os.getenv("FILEWATCHER_API_URL", "http://localhost:8081")

# –°–µ–º–∞—Ñ–æ—Ä—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
PARSE_SEMAPHORE = Semaphore(2)   # –ú–∞–∫—Å–∏–º—É–º 2 –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
EMBED_SEMAPHORE = Semaphore(3)   # –ú–∞–∫—Å–∏–º—É–º 3 embedding –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
LLM_SEMAPHORE = Semaphore(2)     # –ú–∞–∫—Å–∏–º—É–º 2 LLM –∑–∞–ø—Ä–æ—Å–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ


def get_next_file() -> Optional[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ñ–∞–π–ª –∏–∑ –æ—á–µ—Ä–µ–¥–∏ filewatcher"""
    try:
        response = requests.get(f"{FILEWATCHER_API}/api/next-file", timeout=5)
        if response.status_code == 204:
            return None  # –û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞
        response.raise_for_status()
        return response.json()
    except Exception as e:
        logger.error(f"Failed to get next file from filewatcher: {e}")
        return None


def process_deleted_file(file_hash: str, file_path: str) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ deleted —Ñ–∞–π–ª–∞ - —É–¥–∞–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏ –∑–∞–ø–∏—Å–∏
    
    Args:
        file_hash: –•—ç—à —Ñ–∞–π–ª–∞
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
    """
    try:
        chunks_deleted = db.delete_chunks_by_hash(file_hash)
        db.delete_file_by_hash(file_hash)
        logger.info(f"ü™ì Deleted {file_path} and {chunks_deleted} chunks")
        return True
    except Exception as e:
        logger.error(f"Error deleting file {file_path}: {e}")
        return False


def ingest_pipeline(file_hash: str, file_path: str) -> bool:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: –ø–∞—Ä—Å–∏–Ω–≥ ‚Üí —á–∞–Ω–∫–∏–Ω–≥ ‚Üí —ç–º–±–µ–¥–¥–∏–Ω–≥
    
    Args:
        file_hash: –•—ç—à —Ñ–∞–π–ª–∞
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
    """
    logger.info(f"üçé Start ingest pipeline: {file_path} (hash: {file_hash[:8]}...)")
    
    try:
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
        if file_path.lower().endswith('.docx'):
            logger.info(f"üìñ Parsing file: {file_path}")
            with PARSE_SEMAPHORE:
                raw_text = parser_word_old_task({'hash': file_hash, 'path': file_path})
            logger.info(f"‚úÖ Parsed: {len(raw_text) if raw_text else 0} chars")
        else:
            logger.error(f"Unsupported file type: {file_path}")
            db.mark_as_error(file_hash)
            return False

        if not raw_text or not raw_text.strip():
            logger.error(f"Empty parsed text for {file_path}")
            db.mark_as_error(file_hash)
            return False
        
        # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ temp_parsed
        temp_dir = "/home/alpaca/tmp_md"
        temp_file_path = os.path.join(temp_dir, f"{file_path}.md")
        os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)
        
        with open(temp_file_path, "w", encoding="utf-8") as f:
            f.write(raw_text)
        
        # 3. –ß–∞–Ω–∫–∏–Ω–≥
        chunks = chunking(file_path, raw_text)
        
        if not chunks:
            logger.warning(f"No chunks created for {file_path}")
            db.mark_as_error(file_hash)
            return False
        
        # 4. –≠–º–±–µ–¥–¥–∏–Ω–≥ (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏)
        with EMBED_SEMAPHORE:
            chunks_count = embedding(db, file_hash, file_path, chunks)
        
        if chunks_count == 0:
            logger.warning(f"No embeddings created for {file_path}")
            db.mark_as_error(file_hash)
            return False
        
        db.mark_as_ok(file_hash)
        logger.info(f"‚úÖ File processed successfully: {file_path} | chunks={chunks_count}")
        return True
        
    except Exception as e:
        import traceback
        logger.error(f"Pipeline failed for {file_path}: {e}")
        logger.error(f"Traceback:\n{traceback.format_exc()}")
        db.mark_as_error(file_hash)
        return False


def process_file(file_info: Dict[str, Any]) -> bool:
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª
    
    Args:
        file_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –∏–∑ filewatcher
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
    """
    file_path = file_info['file_path']
    file_hash = file_info['file_hash']
    status = file_info['status_sync']
    
    logger.info(f"Processing file: {file_path} (status={status})")
    
    try:
        if status == 'deleted':
            # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º —á–∞–Ω–∫–∏, –ø–æ—Ç–æ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ updated –µ—Å–ª–∏ —ç—Ç–æ –±—ã–ª updated
            return process_deleted_file(file_hash, file_path)
            
        elif status == 'updated':
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏, –∑–∞—Ç–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ
            process_deleted_file(file_hash, file_path)
            return ingest_pipeline(file_hash, file_path)
            
        elif status == 'added':
            # –ù–æ–≤—ã–π —Ñ–∞–π–ª - –ø—Ä–æ—Å—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            return ingest_pipeline(file_hash, file_path)
            
        else:
            logger.warning(f"Unknown status: {status} for {file_path}")
            return False
            
    except Exception as e:
        logger.error(f"‚úó Error processing {file_path}: {e}")
        db.mark_as_error(file_hash)
        return False


def run_worker(poll_interval: int = 10, max_workers: int = 15):
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª worker —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    
    Args:
        poll_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª –æ–ø—Ä–æ—Å–∞ –æ—á–µ—Ä–µ–¥–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        max_workers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    """

    
    processed_count = 0
    
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="worker") as executor:
        futures = {}  # future -> file_path mapping
        
        while True:
            try:
                # –£–¥–∞–ª—è–µ–º –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ
                done_futures = [f for f in list(futures.keys()) if f.done()]
                for future in done_futures:
                    file_path = futures[future]
                    try:
                        success = future.result()
                        if success:
                            processed_count += 1
                            logger.info(f"üìä Total processed: {processed_count}")
                    except Exception as e:
                        logger.error(f"Task failed for {file_path}: {e}")
                    del futures[future]
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–≤–æ–±–æ–¥–Ω—ã–µ —Å–ª–æ—Ç—ã, –±–µ—Ä—ë–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
                while len(futures) < max_workers:
                    file_info = get_next_file()
                    
                    if file_info is None:
                        # –û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞
                        break
                    
                    # –ü–æ–º–µ—á–∞–µ–º —Ñ–∞–π–ª –∫–∞–∫ processed –°–†–ê–ó–£, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                    db.mark_as_processed(file_info['file_hash'])
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
                    future = executor.submit(process_file, file_info)
                    futures[future] = file_info['file_path']
                    logger.info(f"üöÄ Started: {file_info['file_path']} | Active: {len(futures)}/{max_workers}")
                
                # –ï—Å–ª–∏ –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –∏ –æ—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞, –∂–¥—ë–º
                if not futures:
                    logger.debug("Queue is empty, waiting...")
                    time.sleep(poll_interval)
                else:
                    # –ï—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—â–µ
                    time.sleep(0.5)
                    
            except KeyboardInterrupt:
                logger.info("Shutting down worker...")
                logger.info(f"Waiting for {len(futures)} active tasks to complete...")
                # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
                for future in as_completed(futures.keys()):
                    try:
                        future.result()
                    except Exception:
                        pass
                break
            except Exception as e:
                logger.error(f"Unexpected error in worker loop: {e}")
                time.sleep(poll_interval)


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö)
    tests_passed = run_tests_on_startup(settings)
    
    # –ü–æ—Å–ª–µ —Ç–µ—Å—Ç–æ–≤ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    # (—Ç–µ—Å—Ç—ã –æ—á–∏—â–∞—é—Ç handlers –≤ –∫–æ–Ω—Ü–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è)
    setup_logging()
    logger = get_logger("alpaca.worker")
    
    if not tests_passed:
        logger.error("–¢–µ—Å—Ç—ã –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å - –≤—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã")
        exit(1)
    
    run_worker(poll_interval=5, max_workers=5)
